{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sistema de Arbitraje HFT - Pipeline Completo\n",
        "\n",
        "## Índice\n",
        "\n",
        "1. [Introducción](#introduccion)\n",
        "2. [Configuración y Selección de Dataset](#configuracion)\n",
        "3. [Fase 1: Carga de Datos](#fase1)\n",
        "4. [Fase 2: Limpieza de Datos](#fase2)\n",
        "5. [Fase 3: Consolidated Tape](#fase3)\n",
        "6. [Fase 4: Detección de Señales](#fase4)\n",
        "7. [Fase 5: Ejecución Instantánea](#fase5)\n",
        "8. [Fase 6: Análisis Final](#fase6)\n",
        "9. [Visualizaciones y Reportes](#visualizaciones)\n",
        "\n",
        "---\n",
        "\n",
        "## [WARNING] IMPORTANTE: Selección de Dataset\n",
        "\n",
        "**Antes de ejecutar el notebook**, asegúrate de configurar el dataset en la **celda 1**:\n",
        "\n",
        "- **DATA_SMALL**: Para pruebas rápidas (más rápido)\n",
        "- **DATA_BIG**: Para análisis completo (más datos, más lento)\n",
        "\n",
        "Cambia la variable `USE_DATA_BIG = False` a `True` si quieres usar DATA_BIG.\n",
        "\n",
        "---\n",
        "\n",
        "## Introducción {#introduccion}\n",
        "\n",
        "Este notebook implementa un sistema completo de detección de arbitraje en mercados fragmentados europeos.\n",
        "\n",
        "### La Analogía del Mercado de Frutas\n",
        "\n",
        "Imagina que tienes **4 mercados de frutas** (XMAD, AQXE, CEUX, TRQX) donde se venden manzanas.\n",
        "\n",
        "En cada mercado, en cada momento, hay:\n",
        "- Un **VENDEDOR** con el precio **MÁS BAJO** al que está dispuesto a vender (ASK)\n",
        "- Un **COMPRADOR** con el precio **MÁS ALTO** al que está dispuesto a comprar (BID)\n",
        "\n",
        "### La Regla de Oro\n",
        "\n",
        "**UNA OPORTUNIDAD EXISTE CUANDO:**\n",
        "\n",
        "```\n",
        "MAX(todos los bids) > MIN(todos los asks)\n",
        "```\n",
        "\n",
        "Es decir:\n",
        "- El precio **MÁS ALTO** que alguien está dispuesto a **PAGAR** en CUALQUIER mercado\n",
        "- Es **MAYOR** que\n",
        "- El precio **MÁS BAJO** al que alguien está dispuesto a **VENDER** en CUALQUIER mercado\n",
        "\n",
        "Cuando esto pasa → **ARBITRAJE POSIBLE** [OK]\n",
        "\n",
        "### Ejemplo Práctico\n",
        "\n",
        "```\n",
        "Mercado XMAD:  Comprador ofrece 10.52€ | Vendedor pide 10.54€\n",
        "Mercado AQXE:  Comprador ofrece 10.49€ | Vendedor pide 10.51€\n",
        "Mercado CEUX:  Comprador ofrece 10.48€ | Vendedor pide 10.53€\n",
        "Mercado TRQX:  Comprador ofrece 10.50€ | Vendedor pide 10.52€\n",
        "```\n",
        "\n",
        "**Análisis:**\n",
        "- **Mejor comprador:** XMAD (10.52€) ← El que más paga\n",
        "- **Mejor vendedor:** AQXE (10.51€) ← El que menos pide\n",
        "\n",
        "**¡OPORTUNIDAD!** 10.52€ > 10.51€\n",
        "\n",
        "**Tu arbitraje:**\n",
        "1. Compras en AQXE por 10.51€\n",
        "2. Vendes en XMAD por 10.52€\n",
        "3. Ganancia: 0.01€ por manzana\n",
        "\n",
        "Si hay 300 manzanas disponibles → Ganancia total: **3.00€**\n",
        "\n",
        "---\n",
        "\n",
        "## Configuración {#configuracion}\n",
        "\n",
        "### Asunciones del Modelo\n",
        "\n",
        "- [OK] **Latencia = 0** (ejecución instantánea)\n",
        "- [OK] **Sin comisiones** de mercado\n",
        "- [OK] **Profit teórico = Profit real**\n",
        "\n",
        "### Estructura del Pipeline\n",
        "\n",
        "```\n",
        "┌─────────────────┐\n",
        "│  1. CARGA       │  → Leer archivos QTE y STS\n",
        "└────────┬────────┘\n",
        "         │\n",
        "┌────────▼────────┐\n",
        "│  2. LIMPIEZA    │  → Eliminar magic numbers, filtrar por status\n",
        "└────────┬────────┘\n",
        "         │\n",
        "┌────────▼────────┐\n",
        "│  3. CONSOLIDAR  │  → Crear tape único con todos los venues\n",
        "└────────┬────────┘\n",
        "         │\n",
        "┌────────▼────────┐\n",
        "│  4. DETECTAR    │  → Buscar MAX(bid) > MIN(ask)\n",
        "└────────┬────────┘\n",
        "         │\n",
        "┌────────▼────────┐\n",
        "│  5. EJECUTAR    │  → Calcular profit (latencia=0)\n",
        "└────────┬────────┘\n",
        "         │\n",
        "┌────────▼────────┐\n",
        "│  6. ANALIZAR    │  → Generar reportes y visualizaciones\n",
        "└─────────────────┘\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Imports completados\n",
            "[INFO] Directorio de trabajo: c:\\Users\\Pc\\Downloads\\TAREA_RENTA_VARIABLE\n",
            "\n",
            "================================================================================\n",
            "SELECCIÓN DE DATASET\n",
            "================================================================================\n",
            "\n",
            "[INFO] Selecciona el dataset a usar:\n",
            "   1. DATA_SMALL (rápido, para testing)\n",
            "   2. DATA_BIG (completo, para producción)\n",
            "\n",
            "[OK] Seleccionado: DATA_SMALL\n",
            "[INFO] Dataset seleccionado: DATA_SMALL\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS Y CONFIGURACIÓN INICIAL\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar paths\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "print(\"[OK] Imports completados\")\n",
        "print(f\"[INFO] Directorio de trabajo: {PROJECT_ROOT}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SELECTOR DE DATASET\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SELECCIÓN DE DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Función helper para display_dataframe (igual que en main_script.py)\n",
        "def display_dataframe(df: pd.DataFrame, title: str = \"\", max_rows: int = 20):\n",
        "    \"\"\"Muestra un DataFrame de forma clara y legible.\"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        print(f\"\\n  {title}: (vacío)\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n  {'=' * 76}\")\n",
        "    if title:\n",
        "        print(f\"  {title}\")\n",
        "        print(f\"  {'=' * 76}\")\n",
        "    \n",
        "    display_df = df.head(max_rows)\n",
        "    \n",
        "    with pd.option_context('display.max_columns', None,\n",
        "                          'display.width', None,\n",
        "                          'display.max_colwidth', 50):\n",
        "        print(display_df.to_string(index=False))\n",
        "    \n",
        "    if len(df) > max_rows:\n",
        "        print(f\"\\n  ... (mostrando {max_rows} de {len(df)} filas totales)\")\n",
        "    \n",
        "    print(f\"  {'=' * 76}\")\n",
        "\n",
        "# Selección interactiva del dataset\n",
        "print(\"\\n[INFO] Selecciona el dataset a usar:\")\n",
        "print(\"   1. DATA_SMALL (rápido, para testing)\")\n",
        "print(\"   2. DATA_BIG (completo, para producción)\")\n",
        "\n",
        "# Por defecto usar DATA_SMALL, pero se puede cambiar manualmente\n",
        "USE_DATA_BIG = False  # Cambiar a True para usar DATA_BIG\n",
        "\n",
        "if USE_DATA_BIG:\n",
        "    print(\"\\n[OK] Seleccionado: DATA_BIG\")\n",
        "    DATA_DIR_NAME = \"DATA_BIG\"\n",
        "else:\n",
        "    print(\"\\n[OK] Seleccionado: DATA_SMALL\")\n",
        "    DATA_DIR_NAME = \"DATA_SMALL\"\n",
        "\n",
        "print(f\"[INFO] Dataset seleccionado: {DATA_DIR_NAME}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importar Módulos del Sistema\n",
        "\n",
        "A continuación importamos todos los módulos necesarios:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Todos los módulos importados correctamente\n",
            "\n",
            "[INFO] Configuración:\n",
            "   - Dataset seleccionado: DATA_SMALL\n",
            "   - Directorio de datos: c:\\Users\\Pc\\Downloads\\TAREA_RENTA_VARIABLE\\data\\DATA_SMALL\n",
            "   - Directorio de output: c:\\Users\\Pc\\Downloads\\TAREA_RENTA_VARIABLE\\output\n",
            "   - Magic numbers a filtrar: 6 valores\n",
            "   - Venues configurados: ['XMAD', 'AQXE', 'AQEU', 'CEUX', 'TRQX', 'TQEX']\n",
            "\n",
            "[INFO] Limpiando outputs anteriores...\n",
            "[OK] Directorios de output listos\n"
          ]
        }
      ],
      "source": [
        "# Importar todos los módulos del sistema\n",
        "from config_module import config\n",
        "from data_loader_module import DataLoader\n",
        "from data_cleaner_module import DataCleaner\n",
        "from consolidator_module import ConsolidatedTape\n",
        "from signal_generator_module import SignalGenerator\n",
        "from analyzer_module import ArbitrageAnalyzer\n",
        "\n",
        "print(\"[OK] Todos los módulos importados correctamente\")\n",
        "\n",
        "# Configurar directorio de datos según selección\n",
        "if DATA_DIR_NAME == \"DATA_BIG\":\n",
        "    data_dir = config.DATA_BIG_DIR\n",
        "else:\n",
        "    data_dir = config.DATA_SMALL_DIR\n",
        "\n",
        "print(f\"\\n[INFO] Configuración:\")\n",
        "print(f\"   - Dataset seleccionado: {DATA_DIR_NAME}\")\n",
        "print(f\"   - Directorio de datos: {data_dir}\")\n",
        "print(f\"   - Directorio de output: {config.OUTPUT_DIR}\")\n",
        "print(f\"   - Magic numbers a filtrar: {len(config.MAGIC_NUMBERS)} valores\")\n",
        "print(f\"   - Venues configurados: {list(config.VALID_STATES.keys())}\")\n",
        "\n",
        "# Verificar que el directorio existe\n",
        "if not data_dir.exists():\n",
        "    print(f\"\\n[ERROR] Directorio no encontrado: {data_dir}\")\n",
        "    print(\"Por favor, asegúrate de que los datos están en la ubicación correcta\")\n",
        "    raise FileNotFoundError(f\"Directorio no encontrado: {data_dir}\")\n",
        "\n",
        "# Limpiar outputs anteriores (igual que en main_script.py)\n",
        "import shutil\n",
        "print(\"\\n[INFO] Limpiando outputs anteriores...\")\n",
        "if config.OUTPUT_DIR.exists():\n",
        "    for item in config.OUTPUT_DIR.iterdir():\n",
        "        try:\n",
        "            if item.is_file():\n",
        "                item.unlink()\n",
        "            elif item.is_dir():\n",
        "                shutil.rmtree(item)\n",
        "        except Exception as e:\n",
        "            print(f\"  No se pudo eliminar {item}: {e}\")\n",
        "\n",
        "config.OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "config.FIGURES_DIR.mkdir(exist_ok=True)\n",
        "print(\"[OK] Directorios de output listos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FASE 1: CARGA DE DATOS {#fase1}\n",
        "\n",
        "### Objetivo\n",
        "Cargar archivos QTE (quotes) y STS (status) desde archivos comprimidos `.csv.gz`.\n",
        "\n",
        "### Convención de Nombres\n",
        "```\n",
        "<type>_<session>_<isin>_<ticker>_<mic>_<part>.csv.gz\n",
        "\n",
        "Ejemplo:\n",
        "QTE_2024-01-15_ES0113900J37_SAN_XMAD_1.csv.gz\n",
        "STS_2024-01-15_ES0113900J37_SAN_XMAD_1.csv.gz\n",
        "```\n",
        "\n",
        "### Book Identity Key\n",
        "Cada order book se identifica únicamente por:\n",
        "```\n",
        "(session, isin, mic, ticker)\n",
        "```\n",
        "\n",
        "### Columnas Requeridas\n",
        "\n",
        "**QTE (Quotes):**\n",
        "- `epoch` (int64) - Timestamp en nanosegundos UTC\n",
        "- `px_bid_0`, `px_ask_0` (float64) - Precios best bid/ask\n",
        "- `qty_bid_0`, `qty_ask_0` (float64) - Cantidades disponibles\n",
        "- Niveles 1-9 opcionales si existen\n",
        "\n",
        "**STS (Status):**\n",
        "- `epoch` (int64) - Timestamp en nanosegundos UTC\n",
        "- `market_trading_status` (int64) - Código de estado del mercado\n",
        "\n",
        "### Diagrama de Flujo\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────┐\n",
        "│  Descubrir ISINs disponibles        │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  Para cada ISIN:                     │\n",
        "│  ┌──────────────────────────────┐  │\n",
        "│  │ Buscar archivos QTE y STS     │  │\n",
        "│  │ para cada venue (XMAD, AQXE,  │  │\n",
        "│  │ CEUX, TRQX)                    │  │\n",
        "│  └──────────────┬─────────────────┘  │\n",
        "│                 │                     │\n",
        "│  ┌──────────────▼─────────────────┐  │\n",
        "│  │ Leer CSV comprimido             │  │\n",
        "│  │ - Encoding: utf-8, latin-1      │  │\n",
        "│  │ - Separador: ;                  │  │\n",
        "│  │ - Decimal: .                    │  │\n",
        "│  └──────────────┬─────────────────┘  │\n",
        "│                 │                     │\n",
        "│  ┌──────────────▼─────────────────┐  │\n",
        "│  │ Validar columnas requeridas     │  │\n",
        "│  │ - epoch debe ser int64          │  │\n",
        "│  │ - Precios y cantidades float64  │  │\n",
        "│  └──────────────┬─────────────────┘  │\n",
        "│                 │                     │\n",
        "│  ┌──────────────▼─────────────────┐  │\n",
        "│  │ Retornar Dict[mic] ->          │  │\n",
        "│  │   {'qte': DataFrame,            │  │\n",
        "│  │    'sts': DataFrame}            │  │\n",
        "│  └────────────────────────────────┘  │\n",
        "└──────────────────────────────────────┘\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:35,121 - data_loader_module - INFO - DataLoader initialized: c:\\Users\\Pc\\Downloads\\TAREA_RENTA_VARIABLE\\data\\DATA_SMALL\n",
            "2025-12-09 14:23:35,127 - __main__ - INFO - Directorio de datos: c:\\Users\\Pc\\Downloads\\TAREA_RENTA_VARIABLE\\data\\DATA_SMALL\n",
            "2025-12-09 14:23:35,141 - __main__ - INFO - Analizando ISIN: ES0113900J37\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FASE 1: CARGA DE DATOS\n",
            "================================================================================\n",
            "\n",
            "[INFO] Descubriendo ISINs disponibles...\n",
            "\n",
            "================================================================================\n",
            "DESCUBRIENDO ISINs DISPONIBLES\n",
            "================================================================================\n",
            " Encontrados 1 ISINs unicos\n",
            "  Primeros 5: ['ES0113900J37']\n",
            "\n",
            "[INFO] ISIN seleccionado: ES0113900J37\n",
            "\n",
            "[INFO] Cargando datos para ES0113900J37...\n",
            "\n",
            "================================================================================\n",
            "CARGANDO DATOS PARA ISIN: ES0113900J37\n",
            "================================================================================\n",
            "  Archivos QTE encontrados: 4\n",
            "\n",
            "  [PROCESANDO] Venue: AQEU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:36,634 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SANe_AQEU_1.csv.gz: 101,610 filas válidas (de 101,616)\n",
            "2025-12-09 14:23:36,650 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SANe_AQEU_1.csv.gz: 6 filas\n",
            "2025-12-09 14:23:36,693 - data_loader_module - INFO -   [DEBUG] AQEU: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:36,694 - data_loader_module - INFO -   [DEBUG] AQEU: Total columnas: 70\n",
            "2025-12-09 14:23:36,695 - data_loader_module - INFO -   [DEBUG] AQEU: Filas en QTE: 101610\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] AQEU: 101,610 snapshots\n",
            "\n",
            "  [PROCESANDO] Venue: XMAD\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:40,727 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SAN_XMAD_1.csv.gz: 364,903 filas válidas (de 364,912)\n",
            "2025-12-09 14:23:40,769 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SAN_XMAD_1.csv.gz: 7 filas\n",
            "2025-12-09 14:23:40,867 - data_loader_module - INFO -   [DEBUG] XMAD: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:40,868 - data_loader_module - INFO -   [DEBUG] XMAD: Total columnas: 70\n",
            "2025-12-09 14:23:40,870 - data_loader_module - INFO -   [DEBUG] XMAD: Filas en QTE: 364903\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] XMAD: 364,903 snapshots\n",
            "\n",
            "  [PROCESANDO] Venue: CEUX\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:41,654 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SANe_CEUX_1.csv.gz: 78,462 filas válidas (de 78,472)\n",
            "2025-12-09 14:23:41,662 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SANe_CEUX_1.csv.gz: 7 filas\n",
            "2025-12-09 14:23:41,686 - data_loader_module - INFO -   [DEBUG] CEUX: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:41,687 - data_loader_module - INFO -   [DEBUG] CEUX: Total columnas: 70\n",
            "2025-12-09 14:23:41,688 - data_loader_module - INFO -   [DEBUG] CEUX: Filas en QTE: 78462\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] CEUX: 78,462 snapshots\n",
            "\n",
            "  [PROCESANDO] Venue: TQEX\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:42,536 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SANe_TQEX_1.csv.gz: 43,316 filas válidas (de 43,325)\n",
            "2025-12-09 14:23:42,545 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SANe_TQEX_1.csv.gz: 3 filas\n",
            "2025-12-09 14:23:42,560 - data_loader_module - INFO -   [DEBUG] TQEX: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:42,561 - data_loader_module - INFO -   [DEBUG] TQEX: Total columnas: 70\n",
            "2025-12-09 14:23:42,561 - data_loader_module - INFO -   [DEBUG] TQEX: Filas en QTE: 43316\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] TQEX: 43,316 snapshots\n",
            "\n",
            "[EXITO] Venues cargados: ['AQEU', 'XMAD', 'CEUX', 'TQEX']\n",
            "\n",
            "[OK] Datos cargados exitosamente:\n",
            "   Total venues: 4\n",
            "\n",
            "   [INFO] AQEU:\n",
            "      - QTE rows: 101,610\n",
            "      - STS rows: 6\n",
            "      - Epoch range QTE: 1,762,502,416,697,531 - 1,762,533,000,141,495\n",
            "      - Epoch range STS: 1,762,500,600,000,015 - 1,762,533,156,000,006\n",
            "\n",
            "   [INFO] XMAD:\n",
            "      - QTE rows: 364,903\n",
            "      - STS rows: 7\n",
            "      - Epoch range QTE: 1,762,495,202,094,554 - 1,762,534,870,304,556\n",
            "      - Epoch range STS: 1,762,495,202,277,854 - 1,762,533,900,424,604\n",
            "\n",
            "   [INFO] CEUX:\n",
            "      - QTE rows: 78,462\n",
            "      - STS rows: 7\n",
            "      - Epoch range QTE: 1,762,502,416,697,549 - 1,762,533,375,093,153\n",
            "      - Epoch range STS: 1,762,494,604,118,987 - 1,762,538,400,000,017\n",
            "\n",
            "   [INFO] TQEX:\n",
            "      - QTE rows: 43,316\n",
            "      - STS rows: 3\n",
            "      - Epoch range QTE: 1,762,502,416,697,156 - 1,762,533,001,057,722\n",
            "      - Epoch range STS: 1,762,502,399,028,046 - 1,762,533,901,374,607\n",
            "\n",
            "[INFO] Resumen: 4 venues cargados exitosamente\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FASE 1: CARGA DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FASE 1: CARGA DE DATOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Inicializar loader con el directorio seleccionado\n",
        "loader = DataLoader(data_dir)\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(f\"Directorio de datos: {data_dir}\")\n",
        "\n",
        "# Descubrir ISINs disponibles\n",
        "print(\"\\n[INFO] Descubriendo ISINs disponibles...\")\n",
        "isins = loader.discover_isins()\n",
        "\n",
        "if len(isins) == 0:\n",
        "    logger.error(\"No se encontraron ISINs en el directorio\")\n",
        "    raise ValueError(\"No hay datos disponibles\")\n",
        "\n",
        "# Seleccionar primer ISIN para análisis\n",
        "test_isin = isins[0]\n",
        "logger.info(f\"Analizando ISIN: {test_isin}\")\n",
        "print(f\"\\n[INFO] ISIN seleccionado: {test_isin}\")\n",
        "\n",
        "# Cargar datos raw\n",
        "print(f\"\\n[INFO] Cargando datos para {test_isin}...\")\n",
        "raw_data = loader.load_isin_data(test_isin)\n",
        "\n",
        "if len(raw_data) == 0:\n",
        "    logger.error(f\"No se pudieron cargar datos para {test_isin}\")\n",
        "    raise ValueError(\"Error en carga de datos\")\n",
        "\n",
        "# Mostrar resumen detallado de carga\n",
        "print(f\"\\n[OK] Datos cargados exitosamente:\")\n",
        "print(f\"   Total venues: {len(raw_data)}\")\n",
        "for mic, venue_data in raw_data.items():\n",
        "    qte_df = venue_data.get('qte', pd.DataFrame())\n",
        "    sts_df = venue_data.get('sts', pd.DataFrame())\n",
        "    qte_rows = len(qte_df)\n",
        "    sts_rows = len(sts_df)\n",
        "    print(f\"\\n   [INFO] {mic}:\")\n",
        "    print(f\"      - QTE rows: {qte_rows:,}\")\n",
        "    print(f\"      - STS rows: {sts_rows:,}\")\n",
        "    if qte_rows > 0:\n",
        "        print(f\"      - Epoch range QTE: {qte_df['epoch'].min():,} - {qte_df['epoch'].max():,}\")\n",
        "    if sts_rows > 0:\n",
        "        print(f\"      - Epoch range STS: {sts_df['epoch'].min():,} - {sts_df['epoch'].max():,}\")\n",
        "\n",
        "print(f\"\\n[INFO] Resumen: {len(raw_data)} venues cargados exitosamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FASE 2: LIMPIEZA DE DATOS {#fase2}\n",
        "\n",
        "### Objetivo\n",
        "Eliminar datos inválidos y filtrar por estado de mercado.\n",
        "\n",
        "### Pipeline de Limpieza (ORDEN CRÍTICO)\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────┐\n",
        "│  PASO 1: Eliminar Magic Numbers     │\n",
        "│  ─────────────────────────────────  │\n",
        "│  Magic numbers NO son precios reales │\n",
        "│  • 666666.666 → Unquoted/Unknown    │\n",
        "│  • 999999.999 → Market Order        │\n",
        "│  • 999999.989 → At Open Order       │\n",
        "│  • 999999.988 → At Close Order      │\n",
        "│  • 999999.979 → Pegged Order        │\n",
        "│  • 999999.123 → Unquoted/Unknown    │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 2: Filtrar por Trading Status │\n",
        "│  ─────────────────────────────────  │\n",
        "│  Solo Continuous Trading es válido:  │\n",
        "│  • XMAD: [5832713, 5832756]         │\n",
        "│  • AQXE: [5308427]                   │\n",
        "│  • CEUX: [12255233]                  │\n",
        "│  • TRQX: [7608181]                   │\n",
        "│                                      │\n",
        "│  Usa merge_asof con direction=       │\n",
        "│  'backward' para propagar el último │\n",
        "│  estado conocido a cada quote       │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 3: Validar Precios            │\n",
        "│  ─────────────────────────────────  │\n",
        "│  • px_bid_0 > 0 y px_ask_0 > 0      │\n",
        "│  • px_bid_0 < px_ask_0 (no crossed) │\n",
        "│  • qty_bid_0 > 0 y qty_ask_0 > 0    │\n",
        "│  • Precios < 10000 EUR (sanity)     │\n",
        "└─────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Validación del Book Identity Key\n",
        "\n",
        "Antes de hacer merge QTE-STS, verificamos que pertenecen al mismo order book:\n",
        "\n",
        "```\n",
        "Book Key = (session, isin, mic, ticker)\n",
        "\n",
        "Si QTE.session ≠ STS.session → ERROR\n",
        "Si QTE.isin ≠ STS.isin → ERROR\n",
        "Si QTE.ticker ≠ STS.ticker → ERROR\n",
        "```\n",
        "\n",
        "Esto previene joins incorrectos entre diferentes instrumentos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:42,799 - data_cleaner_module - INFO -     Códigos esperados para AQEU: [5308427]\n",
            "2025-12-09 14:23:42,800 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(5308426), np.int64(5308427), np.int64(5308428), np.int64(5308429)]\n",
            "2025-12-09 14:23:42,801 - data_cleaner_module - INFO -     Códigos que coinciden: [5308427]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FASE 2: LIMPIEZA Y VALIDACIÓN\n",
            "================================================================================\n",
            "\n",
            "[INFO] Aplicando pipeline de limpieza...\n",
            "\n",
            "================================================================================\n",
            "LIMPIEZA Y VALIDACIÓN DE DATOS\n",
            "================================================================================\n",
            "\n",
            "  [LIMPIEZA] AQEU...\n",
            "    Snapshots iniciales: 101,610\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:42,867 - data_cleaner_module - INFO -     Snapshots con estado asignado: 101,610 (100.00%)\n",
            "2025-12-09 14:23:42,868 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 0 (0.00%)\n",
            "2025-12-09 14:23:43,080 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n",
            "2025-12-09 14:23:43,082 - data_cleaner_module - INFO -       5308427: 101,559 snapshots ([VALID])\n",
            "2025-12-09 14:23:43,083 - data_cleaner_module - INFO -       5308428: 51 snapshots ([INVALID])\n",
            "2025-12-09 14:23:43,137 - data_cleaner_module - INFO -     [OK] Removed 51 non-trading snapshots (0.05%)\n",
            "2025-12-09 14:23:43,138 - data_cleaner_module - INFO -     [OK] Kept 101,559 continuous trading snapshots (99.95%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] Snapshots finales: 101,559 (99.95% retenido)\n",
            "\n",
            "  [LIMPIEZA] XMAD...\n",
            "    Snapshots iniciales: 364,903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:43,786 - data_cleaner_module - INFO -     Removed 1 magic numbers (0.00%)\n",
            "2025-12-09 14:23:43,790 - data_cleaner_module - INFO -     Códigos esperados para XMAD: [5832713, 5832756]\n",
            "2025-12-09 14:23:43,791 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(5832754), np.int64(5832755), np.int64(5832756), np.int64(5832757), np.int64(5832758), np.int64(5832762), np.int64(5832763)]\n",
            "2025-12-09 14:23:43,795 - data_cleaner_module - INFO -     Códigos que coinciden: [5832756]\n",
            "2025-12-09 14:23:43,957 - data_cleaner_module - INFO -     Snapshots con estado asignado: 364,901 (100.00%)\n",
            "2025-12-09 14:23:43,958 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 1 (0.00%)\n",
            "2025-12-09 14:23:45,496 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n",
            "2025-12-09 14:23:45,503 - data_cleaner_module - INFO -       5832756: 362,896 snapshots ([VALID])\n",
            "2025-12-09 14:23:45,506 - data_cleaner_module - INFO -       5832757: 1,393 snapshots ([INVALID])\n",
            "2025-12-09 14:23:45,507 - data_cleaner_module - INFO -       5832755: 609 snapshots ([INVALID])\n",
            "2025-12-09 14:23:45,507 - data_cleaner_module - INFO -       5832758: 2 snapshots ([INVALID])\n",
            "2025-12-09 14:23:45,509 - data_cleaner_module - INFO -       5832754: 1 snapshots ([INVALID])\n",
            "2025-12-09 14:23:45,838 - data_cleaner_module - INFO -     [OK] Removed 2,006 non-trading snapshots (0.55%)\n",
            "2025-12-09 14:23:45,841 - data_cleaner_module - INFO -     [OK] Kept 362,896 continuous trading snapshots (99.45%)\n",
            "2025-12-09 14:23:46,347 - data_cleaner_module - INFO -     Removed 2 invalid prices (0.00%)\n",
            "2025-12-09 14:23:46,913 - data_cleaner_module - INFO -     Códigos esperados para CEUX: [12255233]\n",
            "2025-12-09 14:23:46,915 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(12255233), np.int64(12255234), np.int64(12255235), np.int64(12255237), np.int64(12255244)]\n",
            "2025-12-09 14:23:46,916 - data_cleaner_module - INFO -     Códigos que coinciden: [12255233]\n",
            "2025-12-09 14:23:46,947 - data_cleaner_module - INFO -     Snapshots con estado asignado: 78,462 (100.00%)\n",
            "2025-12-09 14:23:46,948 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 0 (0.00%)\n",
            "2025-12-09 14:23:47,025 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n",
            "2025-12-09 14:23:47,026 - data_cleaner_module - INFO -       12255233: 78,375 snapshots ([VALID])\n",
            "2025-12-09 14:23:47,028 - data_cleaner_module - INFO -       12255237: 50 snapshots ([INVALID])\n",
            "2025-12-09 14:23:47,028 - data_cleaner_module - INFO -       12255244: 37 snapshots ([INVALID])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] Snapshots finales: 362,894 (99.45% retenido)\n",
            "\n",
            "  [LIMPIEZA] CEUX...\n",
            "    Snapshots iniciales: 78,462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:47,052 - data_cleaner_module - INFO -     [OK] Removed 87 non-trading snapshots (0.11%)\n",
            "2025-12-09 14:23:47,054 - data_cleaner_module - INFO -     [OK] Kept 78,375 continuous trading snapshots (99.89%)\n",
            "2025-12-09 14:23:47,234 - data_cleaner_module - INFO -     Códigos esperados para TQEX: [7608181]\n",
            "2025-12-09 14:23:47,236 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(7608181), np.int64(7608182), np.int64(7608183)]\n",
            "2025-12-09 14:23:47,237 - data_cleaner_module - INFO -     Códigos que coinciden: [7608181]\n",
            "2025-12-09 14:23:47,256 - data_cleaner_module - INFO -     Snapshots con estado asignado: 43,316 (100.00%)\n",
            "2025-12-09 14:23:47,256 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 0 (0.00%)\n",
            "2025-12-09 14:23:47,298 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n",
            "2025-12-09 14:23:47,301 - data_cleaner_module - INFO -       7608181: 43,316 snapshots ([VALID])\n",
            "2025-12-09 14:23:47,400 - data_cleaner_module - INFO - Quality metrics: 588,291 → 586,144 (99.64% retained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] Snapshots finales: 78,375 (99.89% retenido)\n",
            "\n",
            "  [LIMPIEZA] TQEX...\n",
            "    Snapshots iniciales: 43,316\n",
            "    [OK] Snapshots finales: 43,316 (100.00% retenido)\n",
            "\n",
            "  [MÉTRICAS DE CALIDAD AGREGADAS]\n",
            "    Filas originales: 588,291\n",
            "    Eliminadas por magic numbers: 1 (0.00%)\n",
            "    Eliminadas por status inválido: 2,144 (0.36%)\n",
            "    Eliminadas por validaciones: 2 (0.00%)\n",
            "    Filas finales: 586,144 (99.64% retenido)\n",
            "\n",
            "[EXITO] Limpieza completada para 4 venues\n",
            "\n",
            "[OK] Limpieza completada:\n",
            "   AQEU: 101,559 / 101,610 rows (99.95% retenido)\n",
            "   XMAD: 362,894 / 364,903 rows (99.45% retenido)\n",
            "   CEUX: 78,375 / 78,462 rows (99.89% retenido)\n",
            "   TQEX: 43,316 / 43,316 rows (100.00% retenido)\n",
            "\n",
            "[INFO] Venues válidos después de limpieza: 4\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FASE 2: LIMPIEZA DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FASE 2: LIMPIEZA Y VALIDACIÓN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Inicializar cleaner\n",
        "cleaner = DataCleaner()\n",
        "\n",
        "# Aplicar limpieza a todos los venues\n",
        "print(\"\\n[INFO] Aplicando pipeline de limpieza...\")\n",
        "clean_data = cleaner.clean_all_venues(raw_data)\n",
        "\n",
        "if len(clean_data) == 0:\n",
        "    print(\"[ERROR] No quedan datos después de la limpieza\")\n",
        "    raise ValueError(\"Todos los datos fueron eliminados en la limpieza\")\n",
        "\n",
        "# Mostrar resumen de limpieza\n",
        "print(f\"\\n[OK] Limpieza completada:\")\n",
        "for mic in clean_data.keys():\n",
        "    if mic in raw_data:\n",
        "        original_qte = len(raw_data[mic].get('qte', pd.DataFrame()))\n",
        "        cleaned_qte = len(clean_data[mic])\n",
        "        retention = (cleaned_qte / original_qte * 100) if original_qte > 0 else 0\n",
        "        print(f\"   {mic}: {cleaned_qte:,} / {original_qte:,} rows ({retention:.2f}% retenido)\")\n",
        "\n",
        "print(f\"\\n[INFO] Venues válidos después de limpieza: {len(clean_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FASE 3: CONSOLIDATED TAPE {#fase3}\n",
        "\n",
        "### Objetivo\n",
        "Crear un DataFrame único donde cada fila es un instante de tiempo y las columnas contienen los precios de TODOS los venues simultáneamente.\n",
        "\n",
        "### Estructura del Consolidated Tape\n",
        "\n",
        "```\n",
        "| epoch | XMAD_bid | XMAD_ask | XMAD_bid_qty | XMAD_ask_qty |\n",
        "|       | AQXE_bid | AQXE_ask | AQXE_bid_qty | AQXE_ask_qty |\n",
        "|       | CEUX_bid | CEUX_ask | CEUX_bid_qty | CEUX_ask_qty |\n",
        "|       | TRQX_bid | TRQX_ask | TRQX_bid_qty | TRQX_ask_qty |\n",
        "```\n",
        "\n",
        "### Algoritmo\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────┐\n",
        "│  PASO 1: Renombrar columnas         │\n",
        "│  px_bid_0 → {MIC}_bid               │\n",
        "│  px_ask_0 → {MIC}_ask               │\n",
        "│  qty_bid_0 → {MIC}_bid_qty          │\n",
        "│  qty_ask_0 → {MIC}_ask_qty          │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 2: Merge iterativo             │\n",
        "│  ─────────────────────────────────  │\n",
        "│  • Empezar con primer venue          │\n",
        "│  • Outer merge con cada venue        │\n",
        "│  • Usar 'epoch' como key             │\n",
        "│  • Resultado: todos los timestamps   │\n",
        "│    únicos de todos los venues        │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 3: Ordenar por timestamp      │\n",
        "│  sort_values('epoch')               │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 4: Forward Fill (CRÍTICO)      │\n",
        "│  ─────────────────────────────────  │\n",
        "│  Asunción de market microstructure: │\n",
        "│  El último precio conocido sigue    │\n",
        "│  vigente hasta que llegue un update  │\n",
        "│                                      │\n",
        "│  Ejemplo:                            │\n",
        "│  XMAD actualiza en T=100 y T=200    │\n",
        "│  → Precio en T=150 = precio de T=100│\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 5: Eliminar filas iniciales    │\n",
        "│  con NaNs (si todas las columnas    │\n",
        "│  son NaN)                            │\n",
        "└─────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### ¿Por qué Forward Fill?\n",
        "\n",
        "En mercados fragmentados, cada venue actualiza a diferentes frecuencias:\n",
        "- XMAD puede actualizar cada 100ms\n",
        "- AQXE puede actualizar cada 200ms\n",
        "- CEUX puede actualizar cada 150ms\n",
        "\n",
        "Sin forward fill, tendríamos NaNs en cada timestamp donde un venue no actualiza, lo cual haría **imposible comparar precios** entre venues.\n",
        "\n",
        "Con forward fill, asumimos que el último precio conocido sigue vigente hasta el próximo update, que es estándar en análisis de order books.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:47,479 - consolidator_module - INFO - ConsolidatedTape initialized: time_bin=100ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FASE 3: CONSOLIDATED TAPE\n",
            "================================================================================\n",
            "\n",
            "[INFO] Consolidando datos de todos los venues...\n",
            "\n",
            "================================================================================\n",
            "CREANDO CONSOLIDATED TAPE\n",
            "================================================================================\n",
            "  Venues a consolidar: ['AQEU', 'XMAD', 'CEUX', 'TQEX']\n",
            "   AQEU: 306 snapshots preparados\n",
            "   XMAD: 307 snapshots preparados\n",
            "   CEUX: 306 snapshots preparados\n",
            "   TQEX: 307 snapshots preparados\n",
            "\n",
            "  Usando XMAD como base (307 rows)\n",
            "    Merging con TQEX... OK (307 rows)\n",
            "    Merging con AQEU... OK (307 rows)\n",
            "    Merging con CEUX... OK (307 rows)\n",
            "\n",
            "  [OK] Tape consolidado creado: (307, 17)\n",
            "    - Timestamps únicos: 307\n",
            "    - Columnas totales: 17\n",
            "\n",
            "  Aplicando forward fill...\n",
            "    NaNs antes: 0\n",
            "    NaNs después: 0\n",
            "\n",
            "  Tape final: (307, 17)\n",
            "\n",
            "  [ESTADISTICAS DE COBERTURA]\n",
            "    AQEU: 307 rows válidas (100.0% cobertura)\n",
            "    XMAD: 307 rows válidas (100.0% cobertura)\n",
            "    CEUX: 307 rows válidas (100.0% cobertura)\n",
            "    TQEX: 307 rows válidas (100.0% cobertura)\n",
            "\n",
            "[OK] Validando consolidated tape...\n",
            "\n",
            "================================================================================\n",
            "VALIDANDO CONSOLIDATED TAPE\n",
            "================================================================================\n",
            "  [OK] No NaNs después de las primeras 100 filas\n",
            "  [OK] Timestamps monotónicamente crecientes\n",
            "  [OK] No spreads negativos dentro de venues\n",
            "  [OK] Precios razonables (max: €9.02)\n",
            "\n",
            "  [EXITO] VALIDACIÓN EXITOSA\n",
            "\n",
            "[INFO] Estadísticas del Consolidated Tape:\n",
            "   - Total timestamps únicos: 307\n",
            "   - Columnas totales: 17\n",
            "   - Venues incluidos: 4\n",
            "\n",
            "[INFO] Preview del Consolidated Tape (primeras 5 filas):\n",
            "                             epoch  XMAD_bid  XMAD_ask  XMAD_bid_qty  \\\n",
            "epoch                                                                  \n",
            "1762502400000000  1762502400000000     8.997     9.003         786.0   \n",
            "1762502500000000  1762502500000000     8.974     8.979        3093.0   \n",
            "1762502600000000  1762502600000000     8.985     8.989        3363.0   \n",
            "1762502700000000  1762502700000000     8.954     8.957         875.0   \n",
            "1762502800000000  1762502800000000     8.954     8.958         697.0   \n",
            "\n",
            "                  XMAD_ask_qty  TQEX_bid  TQEX_ask  TQEX_bid_qty  TQEX_ask_qty  \n",
            "epoch                                                                           \n",
            "1762502400000000        1206.0     8.990     9.008         367.0         498.0  \n",
            "1762502500000000         892.0     8.972     8.981        1169.0          86.0  \n",
            "1762502600000000        2389.0     8.985     8.989         110.0         747.0  \n",
            "1762502700000000         378.0     8.948     8.957          52.0          42.0  \n",
            "1762502800000000         898.0     8.927     8.962        1094.0         767.0  \n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FASE 3: CONSOLIDATED TAPE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FASE 3: CONSOLIDATED TAPE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Crear consolidador (con redondeo temporal opcional para datasets grandes)\n",
        "tape_builder = ConsolidatedTape(time_bin_ms=100)  # 100ms bins\n",
        "\n",
        "print(\"\\n[INFO] Consolidando datos de todos los venues...\")\n",
        "consolidated_tape = tape_builder.create_tape(clean_data)\n",
        "\n",
        "if consolidated_tape is None or len(consolidated_tape) == 0:\n",
        "    print(\"[ERROR] Consolidated tape vacío o nulo\")\n",
        "    raise ValueError(\"Error creando consolidated tape\")\n",
        "\n",
        "# Validar tape\n",
        "print(\"\\n[OK] Validando consolidated tape...\")\n",
        "is_valid = tape_builder.validate_tape(consolidated_tape)\n",
        "\n",
        "if not is_valid:\n",
        "    print(\"[ERROR] Consolidated tape falló la validación\")\n",
        "    raise ValueError(\"Tape inválido\")\n",
        "\n",
        "# Mostrar estadísticas\n",
        "print(f\"\\n[INFO] Estadísticas del Consolidated Tape:\")\n",
        "print(f\"   - Total timestamps únicos: {len(consolidated_tape):,}\")\n",
        "print(f\"   - Columnas totales: {len(consolidated_tape.columns)}\")\n",
        "print(f\"   - Venues incluidos: {len(clean_data)}\")\n",
        "\n",
        "# Mostrar preview\n",
        "print(f\"\\n[INFO] Preview del Consolidated Tape (primeras 5 filas):\")\n",
        "display_cols = ['epoch'] + [col for col in consolidated_tape.columns if '_bid' in col or '_ask' in col][:8]\n",
        "print(consolidated_tape[display_cols].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FASE 4: DETECCIÓN DE SEÑALES {#fase4}\n",
        "\n",
        "### Objetivo\n",
        "Detectar oportunidades donde `MAX(bid) > MIN(ask)`.\n",
        "\n",
        "### Algoritmo Paso a Paso\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────┐\n",
        "│  PASO 1: Identificar columnas       │\n",
        "│  • bid_cols = [XMAD_bid, AQXE_bid,  │\n",
        "│                CEUX_bid, TRQX_bid]  │\n",
        "│  • ask_cols = [XMAD_ask, AQXE_ask,  │\n",
        "│                CEUX_ask, TRQX_ask]  │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 2: Calcular Global Max Bid   │\n",
        "│  ─────────────────────────────────  │\n",
        "│  En cada momento:                    │\n",
        "│  max_bid = MAX(todos los bids)      │\n",
        "│  venue_max_bid = venue con max_bid  │\n",
        "│                                      │\n",
        "│  Ejemplo:                            │\n",
        "│  XMAD: 10.52€                        │\n",
        "│  AQXE: 10.49€                        │\n",
        "│  CEUX: 10.48€                        │\n",
        "│  TRQX: 10.50€                        │\n",
        "│  → max_bid = 10.52€ (XMAD)          │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 3: Calcular Global Min Ask    │\n",
        "│  ─────────────────────────────────  │\n",
        "│  En cada momento:                    │\n",
        "│  min_ask = MIN(todos los asks)      │\n",
        "│  venue_min_ask = venue con min_ask  │\n",
        "│                                      │\n",
        "│  Ejemplo:                            │\n",
        "│  XMAD: 10.54€                        │\n",
        "│  AQXE: 10.51€                        │\n",
        "│  CEUX: 10.53€                        │\n",
        "│  TRQX: 10.52€                        │\n",
        "│  → min_ask = 10.51€ (AQXE)          │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 4: Aplicar Regla de Oro       │\n",
        "│  ─────────────────────────────────  │\n",
        "│  signal = 1 si max_bid > min_ask    │\n",
        "│  signal = 0 si no                   │\n",
        "│                                      │\n",
        "│  Ejemplo:                            │\n",
        "│  max_bid = 10.52€ (XMAD)            │\n",
        "│  min_ask = 10.51€ (AQXE)            │\n",
        "│  10.52€ > 10.51€ → [OK] Oportunidad!  │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 5: Calcular Cantidades        │\n",
        "│  ─────────────────────────────────  │\n",
        "│  • Extraer qty del venue_max_bid    │\n",
        "│  • Extraer qty del venue_min_ask    │\n",
        "│  • executable_qty = min(bid_qty,    │\n",
        "│                        ask_qty)     │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 6: Calcular Profit            │\n",
        "│  ─────────────────────────────────  │\n",
        "│  theoretical_profit = max_bid -     │\n",
        "│                       min_ask        │\n",
        "│  total_profit = theoretical_profit  │\n",
        "│                  × executable_qty    │\n",
        "└──────────────┬──────────────────────┘\n",
        "               │\n",
        "┌──────────────▼──────────────────────┐\n",
        "│  PASO 7: Rising Edge Detection      │\n",
        "│  ─────────────────────────────────  │\n",
        "│  Solo contar la PRIMERA aparición   │\n",
        "│  de cada oportunidad continua       │\n",
        "│                                      │\n",
        "│  Si una oportunidad persiste 1000   │\n",
        "│  snapshots, solo la contamos UNA vez│\n",
        "└─────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Rising Edge Detection\n",
        "\n",
        "**Problema:** Si una oportunidad persiste durante muchos snapshots, no queremos contarla múltiples veces.\n",
        "\n",
        "**Solución:** Rising Edge Detection identifica solo la **primera aparición** de cada oportunidad continua.\n",
        "\n",
        "```\n",
        "Snapshot 1: signal = 0 → No oportunidad\n",
        "Snapshot 2: signal = 1 → RISING EDGE (nueva oportunidad)\n",
        "Snapshot 3: signal = 1 → No (continuación de oportunidad anterior)\n",
        "Snapshot 4: signal = 1 → No (continuación)\n",
        "Snapshot 5: signal = 0 → Oportunidad desapareció\n",
        "Snapshot 6: signal = 1 → RISING EDGE (nueva oportunidad)\n",
        "```\n",
        "\n",
        "**Algoritmo:**\n",
        "```python\n",
        "prev_signal = signal.shift(1, fill_value=0)\n",
        "is_rising_edge = (signal == 1) & (prev_signal == 0)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:47,801 - signal_generator_module - INFO - No se encontraron oportunidades de arbitraje\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FASE 4: DETECCIÓN DE SEÑALES DE ARBITRAJE\n",
            "================================================================================\n",
            "\n",
            "[INFO] Detectando oportunidades de arbitraje...\n",
            "[ERROR] No se detectaron señales\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FASE 4: DETECCIÓN DE SEÑALES DE ARBITRAJE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FASE 4: DETECCIÓN DE SEÑALES DE ARBITRAJE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Inicializar generador de señales\n",
        "signal_gen = SignalGenerator()\n",
        "\n",
        "# Inicializar lista de trades ejecutados (vacía al inicio)\n",
        "executed_trades = []\n",
        "\n",
        "# Detectar oportunidades\n",
        "print(\"\\n[INFO] Detectando oportunidades de arbitraje...\")\n",
        "signals_df = signal_gen.detect_opportunities(\n",
        "    consolidated_tape,\n",
        "    executed_trades=executed_trades,\n",
        "    isin=test_isin\n",
        ")\n",
        "\n",
        "if signals_df is None or len(signals_df) == 0:\n",
        "    print(\"[ERROR] No se detectaron señales\")\n",
        "    signals_df = pd.DataFrame()\n",
        "else:\n",
        "    # Mostrar resumen\n",
        "    rising_edges = signals_df[signals_df['is_rising_edge']]\n",
        "    total_opps = len(rising_edges)\n",
        "    \n",
        "    print(f\"\\n[OK] Señales detectadas:\")\n",
        "    print(f\"   - Total snapshots analizados: {len(signals_df):,}\")\n",
        "    print(f\"   - Snapshots con arbitraje: {signals_df['signal'].sum():,}\")\n",
        "    print(f\"   - Rising edges (oportunidades únicas): {total_opps:,}\")\n",
        "    \n",
        "    if total_opps > 0:\n",
        "        total_profit = rising_edges['total_profit'].sum()\n",
        "        avg_profit = rising_edges['total_profit'].mean()\n",
        "        print(f\"   - Profit teórico total: €{total_profit:,.2f}\")\n",
        "        print(f\"   - Profit medio por oportunidad: €{avg_profit:.2f}\")\n",
        "        \n",
        "        # Mostrar primeras oportunidades\n",
        "        print(f\"\\n[INFO] Primeras 5 oportunidades detectadas:\")\n",
        "        opp_cols = ['epoch', 'venue_max_bid', 'venue_min_ask', \n",
        "                   'executable_qty', 'theoretical_profit', 'total_profit']\n",
        "        available_cols = [c for c in opp_cols if c in rising_edges.columns]\n",
        "        print(rising_edges[available_cols].head().to_string(index=False))\n",
        "    \n",
        "    # Analizar pares de venues\n",
        "    print(\"\\n[INFO] Analizando pares de venues...\")\n",
        "    venue_pairs = signal_gen.analyze_venue_pairs(signals_df)\n",
        "    if venue_pairs is not None and len(venue_pairs) > 0:\n",
        "        print(\"\\n[INFO] Top pares de venues por profit:\")\n",
        "        print(venue_pairs.head().to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FASE 5: EJECUCIÓN INSTANTÁNEA {#fase5}\n",
        "\n",
        "### Objetivo\n",
        "Calcular el profit real asumiendo ejecución instantánea (latencia = 0, sin comisiones).\n",
        "\n",
        "### Asunciones del Modelo\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────┐\n",
        "│  Latencia = 0                        │\n",
        "│  ─────────────────────────────────  │\n",
        "│  • Detección y ejecución simultáneas │\n",
        "│  • No hay delay entre detectar y     │\n",
        "│    ejecutar                          │\n",
        "│  • execution_epoch = signal_epoch    │\n",
        "└─────────────────────────────────────┘\n",
        "\n",
        "┌─────────────────────────────────────┐\n",
        "│  Sin Comisiones                      │\n",
        "│  ─────────────────────────────────  │\n",
        "│  • No hay costos de trading          │\n",
        "│  • No hay fees de exchange           │\n",
        "│  • Profit teórico = Profit real      │\n",
        "└─────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Cálculo de Profit Real\n",
        "\n",
        "```\n",
        "real_profit = theoretical_profit\n",
        "real_total_profit = total_profit\n",
        "profit_category = 'Profitable' (todas)\n",
        "```\n",
        "\n",
        "**Nota:** En un modelo más realista, habría que:\n",
        "1. Simular latencia (time machine)\n",
        "2. Recalcular precios en el momento de ejecución\n",
        "3. Aplicar comisiones\n",
        "4. Verificar que la oportunidad sigue existiendo\n",
        "\n",
        "Pero en este modelo simplificado, asumimos ejecución perfecta e instantánea.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FASE 5: EJECUCIÓN INSTANTÁNEA\n",
            "================================================================================\n",
            "\n",
            "[INFO] Asunciones del modelo:\n",
            "   • Latencia = 0 (ejecución instantánea)\n",
            "   • Sin comisiones de mercado\n",
            "   • Profit teórico = Profit real\n",
            "[WARNING] No hay señales para ejecutar\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FASE 5: EJECUCIÓN INSTANTÁNEA (LATENCIA = 0, SIN COMISIONES)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FASE 5: EJECUCIÓN INSTANTÁNEA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n[INFO] Asunciones del modelo:\")\n",
        "print(\"   • Latencia = 0 (ejecución instantánea)\")\n",
        "print(\"   • Sin comisiones de mercado\")\n",
        "print(\"   • Profit teórico = Profit real\")\n",
        "\n",
        "exec_df = None\n",
        "\n",
        "if signals_df is not None and len(signals_df) > 0:\n",
        "    # Filtrar solo rising edges (oportunidades únicas)\n",
        "    rising_edges = signals_df[signals_df['is_rising_edge']]\n",
        "    \n",
        "    if len(rising_edges) > 0:\n",
        "        # Crear DataFrame de ejecuciones\n",
        "        exec_df = rising_edges.copy()\n",
        "        exec_df['execution_epoch'] = exec_df['epoch']  # Ejecución instantánea\n",
        "        exec_df['executed_qty'] = exec_df['executable_qty']\n",
        "        exec_df['real_profit'] = exec_df['theoretical_profit']  # Sin pérdida por latencia\n",
        "        exec_df['real_total_profit'] = exec_df['total_profit']  # Profit total real\n",
        "        exec_df['profit_category'] = 'Profitable'  # Todas son profitable\n",
        "        \n",
        "        print(f\"\\n[OK] Ejecuciones simuladas:\")\n",
        "        print(f\"   - Total oportunidades ejecutables: {len(exec_df):,}\")\n",
        "        print(f\"   - Profit total real: €{exec_df['real_total_profit'].sum():,.2f}\")\n",
        "        \n",
        "        # Mostrar primeras ejecuciones\n",
        "        print(f\"\\n[INFO] Primeras 5 ejecuciones:\")\n",
        "        exec_cols = ['epoch', 'execution_epoch', 'venue_max_bid', 'venue_min_ask',\n",
        "                    'executed_qty', 'real_profit', 'real_total_profit']\n",
        "        available_cols = [c for c in exec_cols if c in exec_df.columns]\n",
        "        print(exec_df[available_cols].head().to_string(index=False))\n",
        "    else:\n",
        "        print(\"[WARNING] No hay rising edges para ejecutar\")\n",
        "else:\n",
        "    print(\"[WARNING] No hay señales para ejecutar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## FASE 6: ANÁLISIS FINAL {#fase6}\n",
        "\n",
        "### Objetivo\n",
        "Generar métricas agregadas y estadísticas descriptivas.\n",
        "\n",
        "### Métricas Calculadas\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────┐\n",
        "│  Métricas por Oportunidad           │\n",
        "│  ─────────────────────────────────  │\n",
        "│  • Total oportunidades detectadas   │\n",
        "│  • Total profit teórico              │\n",
        "│  • Profit medio por oportunidad     │\n",
        "│  • Profit máximo                     │\n",
        "│  • Cantidad media ejecutable         │\n",
        "└─────────────────────────────────────┘\n",
        "\n",
        "┌─────────────────────────────────────┐\n",
        "│  Métricas de ROI                    │\n",
        "│  ─────────────────────────────────  │\n",
        "│  • Capital requerido                │\n",
        "│  • Profit total                     │\n",
        "│  • ROI porcentual                   │\n",
        "│  • Profit por unidad de capital     │\n",
        "└─────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Análisis de Pares de Venues\n",
        "\n",
        "Identifica qué combinaciones de venues generan más oportunidades:\n",
        "\n",
        "```\n",
        "Ejemplo:\n",
        "Buy@AQXE / Sell@XMAD: 150 oportunidades, €450.00 total profit\n",
        "Buy@CEUX / Sell@XMAD: 89 oportunidades, €234.50 total profit\n",
        "...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FASE 6: ANÁLISIS FINAL\n",
            "================================================================================\n",
            "[WARNING] No hay datos suficientes para análisis final\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FASE 6: ANÁLISIS FINAL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FASE 6: ANÁLISIS FINAL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "metrics = None\n",
        "roi_metrics = None\n",
        "\n",
        "if signals_df is not None and exec_df is not None and len(exec_df) > 0:\n",
        "    analyzer = ArbitrageAnalyzer()\n",
        "    \n",
        "    print(\"\\n[INFO] Analizando oportunidades...\")\n",
        "    metrics = analyzer.analyze_opportunities(signals_df, exec_df)\n",
        "    \n",
        "    print(\"\\n[INFO] Estimando ROI...\")\n",
        "    # Sin comisiones: trading_costs_bps = 0\n",
        "    roi_metrics = analyzer.estimate_roi(metrics, trading_costs_bps=0.0, capital_eur=100000)\n",
        "    \n",
        "    # Mostrar métricas\n",
        "    if metrics:\n",
        "        print(\"\\n[OK] Métricas del análisis:\")\n",
        "        metrics_df = pd.DataFrame([metrics])\n",
        "        print(metrics_df.to_string(index=False))\n",
        "    \n",
        "    if roi_metrics:\n",
        "        print(\"\\n[OK] Métricas de ROI:\")\n",
        "        roi_df = pd.DataFrame([roi_metrics])\n",
        "        print(roi_df.to_string(index=False))\n",
        "    \n",
        "    # Generar reporte\n",
        "    print(\"\\n[INFO] Generando reporte final...\")\n",
        "    analyzer.generate_summary_report(\n",
        "        metrics,\n",
        "        roi_metrics,\n",
        "        output_path=config.OUTPUT_DIR / f'report_{test_isin}.txt'\n",
        "    )\n",
        "    print(\"[OK] Reporte generado\")\n",
        "else:\n",
        "    print(\"[WARNING] No hay datos suficientes para análisis final\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## VISUALIZACIONES Y REPORTES {#visualizaciones}\n",
        "\n",
        "### Gráficas Generadas\n",
        "\n",
        "1. **Consolidated Tape**: Muestra la evolución de precios bid/ask de todos los venues\n",
        "2. **Señales Detectadas**: Visualiza las oportunidades de arbitraje en el tiempo\n",
        "3. **Análisis de Pares**: Gráfico de barras con profit por par de venues\n",
        "\n",
        "### Archivos Generados\n",
        "\n",
        "- `opportunities_{ISIN}.csv`: Todas las oportunidades detectadas\n",
        "- `execution_{ISIN}.csv`: Todas las ejecuciones simuladas\n",
        "- `report_{ISIN}.txt`: Reporte de métricas y ROI\n",
        "- `complete_report_{ISIN}.md`: Documento Markdown completo\n",
        "- `figures/`: Directorio con todas las gráficas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "VISUALIZACIONES Y EXPORTACIÓN\n",
            "================================================================================\n",
            "[WARNING] No hay señales para visualizar\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZACIONES Y EXPORTACIÓN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VISUALIZACIONES Y EXPORTACIÓN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if signals_df is not None and len(signals_df) > 0:\n",
        "    # Visualizar señales\n",
        "    print(\"\\n[INFO] Generando visualizaciones...\")\n",
        "    signal_gen.visualize_signals(signals_df, test_isin)\n",
        "    \n",
        "    # Visualizar consolidated tape\n",
        "    tape_builder.visualize_tape(consolidated_tape, test_isin)\n",
        "    \n",
        "    # Exportar oportunidades\n",
        "    print(\"\\n[INFO] Exportando oportunidades...\")\n",
        "    signal_gen.export_opportunities(\n",
        "        signals_df,\n",
        "        output_path=config.OUTPUT_DIR / f\"opportunities_{test_isin}.csv\"\n",
        "    )\n",
        "    \n",
        "    # Exportar ejecuciones\n",
        "    if exec_df is not None and len(exec_df) > 0:\n",
        "        exec_cols = ['epoch', 'execution_epoch', 'venue_max_bid', 'venue_min_ask',\n",
        "                    'executed_qty', 'real_profit', 'real_total_profit', 'profit_category']\n",
        "        exec_df[[c for c in exec_cols if c in exec_df.columns]].to_csv(\n",
        "            config.OUTPUT_DIR / f\"execution_{test_isin}.csv\",\n",
        "            index=False\n",
        "        )\n",
        "        print(\"[OK] Ejecuciones exportadas\")\n",
        "    \n",
        "    print(\"\\n[OK] Visualizaciones generadas\")\n",
        "    print(f\"   [INFO] Archivos guardados en: {config.OUTPUT_DIR}\")\n",
        "    print(f\"   [INFO] Gráficas guardadas en: {config.FIGURES_DIR}\")\n",
        "else:\n",
        "    print(\"[WARNING] No hay señales para visualizar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## RESUMEN FINAL\n",
        "\n",
        "### Resultados del Análisis\n",
        "\n",
        "A continuación se muestra un resumen completo de todos los resultados obtenidos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "RESUMEN DEL ANÁLISIS\n",
            "================================================================================\n",
            "\n",
            "[INFO] ISIN: ES0113900J37\n",
            "   - Dataset usado: DATA_SMALL\n",
            "   - Total snapshots en tape: 307\n",
            "   - Venues incluidos: 4\n",
            "\n",
            "================================================================================\n",
            "[ÉXITO] ANÁLISIS COMPLETADO CON ÉXITO\n",
            "================================================================================\n",
            "\n",
            "[INFO] Archivos generados:\n",
            "   - Log: c:\\Users\\Pc\\Downloads\\TAREA_RENTA_VARIABLE\\output\\arbitrage_system.log\n",
            "   - Figuras: c:\\Users\\Pc\\Downloads\\TAREA_RENTA_VARIABLE\\output\\figures\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RESUMEN FINAL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RESUMEN DEL ANÁLISIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n[INFO] ISIN: {test_isin}\")\n",
        "print(f\"   - Dataset usado: {DATA_DIR_NAME}\")\n",
        "print(f\"   - Total snapshots en tape: {len(consolidated_tape):,}\")\n",
        "print(f\"   - Venues incluidos: {len(clean_data)}\")\n",
        "\n",
        "if signals_df is not None and len(signals_df) > 0:\n",
        "    total_opportunities = signals_df['is_rising_edge'].sum()\n",
        "    total_profit = signals_df[signals_df['is_rising_edge']]['total_profit'].sum()\n",
        "    \n",
        "    print(f\"\\n[INFO] Oportunidades:\")\n",
        "    print(f\"   - Detectadas: {total_opportunities:,}\")\n",
        "    print(f\"   - Profit total (ejecución instantánea): €{total_profit:,.2f}\")\n",
        "    \n",
        "    if total_opportunities > 0:\n",
        "        avg_profit = signals_df[signals_df['is_rising_edge']]['total_profit'].mean()\n",
        "        print(f\"   - Profit medio por oportunidad: €{avg_profit:.2f}\")\n",
        "\n",
        "if exec_df is not None and len(exec_df) > 0:\n",
        "    profitable_ops = len(exec_df)\n",
        "    real_profit = exec_df['real_total_profit'].sum()\n",
        "    print(f\"\\n[INFO] Ejecuciones:\")\n",
        "    print(f\"   - Oportunidades ejecutadas: {profitable_ops:,}\")\n",
        "    print(f\"   - Profit real total: €{real_profit:,.2f}\")\n",
        "\n",
        "if roi_metrics:\n",
        "    print(f\"\\n[INFO] ROI:\")\n",
        "    print(f\"   - ROI estimado: {roi_metrics.get('roi_pct', 0):.4f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[ÉXITO] ANÁLISIS COMPLETADO CON ÉXITO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Mostrar archivos generados\n",
        "print(f\"\\n[INFO] Archivos generados:\")\n",
        "print(f\"   - Log: {config.OUTPUT_DIR / 'arbitrage_system.log'}\")\n",
        "if signals_df is not None and len(signals_df) > 0:\n",
        "    print(f\"   - Oportunidades: {config.OUTPUT_DIR / f'opportunities_{test_isin}.csv'}\")\n",
        "if exec_df is not None and len(exec_df) > 0:\n",
        "    print(f\"   - Ejecuciones: {config.OUTPUT_DIR / f'execution_{test_isin}.csv'}\")\n",
        "if metrics:\n",
        "    print(f\"   - Reporte: {config.OUTPUT_DIR / f'report_{test_isin}.txt'}\")\n",
        "print(f\"   - Figuras: {config.FIGURES_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## EXPLORACIÓN ADICIONAL\n",
        "\n",
        "### Visualizar Datos Intermedios\n",
        "\n",
        "Puedes explorar los datos en cualquier punto del pipeline usando las variables:\n",
        "\n",
        "- `raw_data`: Datos después de la carga\n",
        "- `clean_data`: Datos después de la limpieza\n",
        "- `consolidated_tape`: Tape consolidado con todos los venues\n",
        "- `signals_df`: DataFrame con todas las señales detectadas\n",
        "- `exec_df`: DataFrame con todas las ejecuciones simuladas\n",
        "\n",
        "### Ejemplos de Exploración\n",
        "\n",
        "```python\n",
        "# Ver distribución de precios en el consolidated tape\n",
        "consolidated_tape[['XMAD_bid', 'XMAD_ask', 'AQXE_bid', 'AQXE_ask']].describe()\n",
        "\n",
        "# Ver oportunidades por venue pair\n",
        "signals_df[signals_df['is_rising_edge']].groupby(['venue_max_bid', 'venue_min_ask']).size()\n",
        "\n",
        "# Ver evolución temporal del profit\n",
        "signals_df[signals_df['is_rising_edge']].plot(x='epoch', y='total_profit', kind='line')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Usa las celdas anteriores para explorar los datos en detalle\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# EXPLORACIÓN ADICIONAL - Celdas opcionales para análisis personalizado\n",
        "# ============================================================================\n",
        "\n",
        "# Descomenta las siguientes líneas para explorar los datos:\n",
        "\n",
        "# 1. Ver estadísticas del consolidated tape\n",
        "# print(\"\\n[INFO] Estadísticas de precios en Consolidated Tape:\")\n",
        "# price_cols = [col for col in consolidated_tape.columns if '_bid' in col or '_ask' in col]\n",
        "# print(consolidated_tape[price_cols].describe())\n",
        "\n",
        "# 2. Ver distribución de oportunidades por venue pair\n",
        "# if signals_df is not None and len(signals_df) > 0:\n",
        "#     rising_edges = signals_df[signals_df['is_rising_edge']]\n",
        "#     print(\"\\n[INFO] Oportunidades por par de venues:\")\n",
        "#     venue_pairs = rising_edges.groupby(['venue_max_bid', 'venue_min_ask']).agg({\n",
        "#         'total_profit': ['count', 'sum', 'mean']\n",
        "#     })\n",
        "#     print(venue_pairs)\n",
        "\n",
        "# 3. Ver evolución temporal del profit\n",
        "# if signals_df is not None and len(signals_df) > 0:\n",
        "#     rising_edges = signals_df[signals_df['is_rising_edge']]\n",
        "#     plt.figure(figsize=(12, 6))\n",
        "#     plt.plot(rising_edges['epoch'], rising_edges['total_profit'], marker='o', markersize=2)\n",
        "#     plt.xlabel('Epoch (nanosegundos)')\n",
        "#     plt.ylabel('Profit (€)')\n",
        "#     plt.title('Evolución Temporal del Profit')\n",
        "#     plt.grid(True)\n",
        "#     plt.show()\n",
        "\n",
        "print(\"\\n[INFO] Usa las celdas anteriores para explorar los datos en detalle\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ANÁLISIS COMPLETO DE TODOS LOS ISINs (Igual que el otro código)\n",
        "\n",
        "Esta sección procesa TODOS los ISINs disponibles y genera:\n",
        "1. **Money Table**: Tabla de profit por ISIN y latencia\n",
        "2. **Decay Chart**: Gráfico de decay de profit con latencia\n",
        "3. **Top Opportunities**: Top 5 ISINs más rentables\n",
        "4. **Summary**: Respuestas a las preguntas clave\n",
        "\n",
        "**Flujo igual al otro código:**\n",
        "- Descubrir todos los ISINs\n",
        "- Para cada ISIN: cargar → consolidar → detectar → rising edge → simular latencias\n",
        "- Generar tablas y gráficos finales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ANÁLISIS COMPLETO DE TODOS LOS ISINs\n",
            "================================================================================\n",
            "\n",
            "[INFO] Descubriendo ISINs...\n",
            "\n",
            "================================================================================\n",
            "DESCUBRIENDO ISINs DISPONIBLES\n",
            "================================================================================\n",
            " Encontrados 1 ISINs unicos\n",
            "  Primeros 5: ['ES0113900J37']\n",
            "Found 1 unique ISINs\n",
            "\n",
            "[1/1] Processing ISIN: ES0113900J37\n",
            "\n",
            "================================================================================\n",
            "CARGANDO DATOS PARA ISIN: ES0113900J37\n",
            "================================================================================\n",
            "  Archivos QTE encontrados: 4\n",
            "\n",
            "  [PROCESANDO] Venue: AQEU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:49,533 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SANe_AQEU_1.csv.gz: 101,610 filas válidas (de 101,616)\n",
            "2025-12-09 14:23:49,544 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SANe_AQEU_1.csv.gz: 6 filas\n",
            "2025-12-09 14:23:49,587 - data_loader_module - INFO -   [DEBUG] AQEU: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:49,590 - data_loader_module - INFO -   [DEBUG] AQEU: Total columnas: 70\n",
            "2025-12-09 14:23:49,590 - data_loader_module - INFO -   [DEBUG] AQEU: Filas en QTE: 101610\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] AQEU: 101,610 snapshots\n",
            "\n",
            "  [PROCESANDO] Venue: XMAD\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:53,594 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SAN_XMAD_1.csv.gz: 364,903 filas válidas (de 364,912)\n",
            "2025-12-09 14:23:53,614 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SAN_XMAD_1.csv.gz: 7 filas\n",
            "2025-12-09 14:23:53,788 - data_loader_module - INFO -   [DEBUG] XMAD: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:53,789 - data_loader_module - INFO -   [DEBUG] XMAD: Total columnas: 70\n",
            "2025-12-09 14:23:53,790 - data_loader_module - INFO -   [DEBUG] XMAD: Filas en QTE: 364903\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] XMAD: 364,903 snapshots\n",
            "\n",
            "  [PROCESANDO] Venue: CEUX\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:56,276 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SANe_CEUX_1.csv.gz: 78,462 filas válidas (de 78,472)\n",
            "2025-12-09 14:23:56,284 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SANe_CEUX_1.csv.gz: 7 filas\n",
            "2025-12-09 14:23:56,310 - data_loader_module - INFO -   [DEBUG] CEUX: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:56,311 - data_loader_module - INFO -   [DEBUG] CEUX: Total columnas: 70\n",
            "2025-12-09 14:23:56,311 - data_loader_module - INFO -   [DEBUG] CEUX: Filas en QTE: 78462\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] CEUX: 78,462 snapshots\n",
            "\n",
            "  [PROCESANDO] Venue: TQEX\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:56,855 - data_loader_module - INFO -   [OK] QTE_2025-11-07_ES0113900J37_SANe_TQEX_1.csv.gz: 43,316 filas válidas (de 43,325)\n",
            "2025-12-09 14:23:56,859 - data_loader_module - INFO -   [OK] STS_2025-11-07_ES0113900J37_SANe_TQEX_1.csv.gz: 3 filas\n",
            "2025-12-09 14:23:56,873 - data_loader_module - INFO -   [DEBUG] TQEX: Columnas disponibles en QTE: ['session', 'inst_id', 'sequence', 'isin', 'ticker', 'mic', 'currency', 'epoch', 'event_timestamp', 'bloombergTicker', 'ord_bid_0', 'qty_bid_0', 'px_bid_0', 'px_ask_0', 'qty_ask_0', 'ord_ask_0', 'ord_bid_1', 'qty_bid_1', 'px_bid_1', 'px_ask_1']\n",
            "2025-12-09 14:23:56,874 - data_loader_module - INFO -   [DEBUG] TQEX: Total columnas: 70\n",
            "2025-12-09 14:23:56,875 - data_loader_module - INFO -   [DEBUG] TQEX: Filas en QTE: 43316\n",
            "2025-12-09 14:23:56,948 - data_cleaner_module - INFO -     Códigos esperados para AQEU: [5308427]\n",
            "2025-12-09 14:23:56,949 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(5308426), np.int64(5308427), np.int64(5308428), np.int64(5308429)]\n",
            "2025-12-09 14:23:56,950 - data_cleaner_module - INFO -     Códigos que coinciden: [5308427]\n",
            "2025-12-09 14:23:56,995 - data_cleaner_module - INFO -     Snapshots con estado asignado: 101,610 (100.00%)\n",
            "2025-12-09 14:23:56,995 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 0 (0.00%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] TQEX: 43,316 snapshots\n",
            "\n",
            "[EXITO] Venues cargados: ['AQEU', 'XMAD', 'CEUX', 'TQEX']\n",
            "  Found data from 4 exchange(s): ['AQEU', 'XMAD', 'CEUX', 'TQEX']\n",
            "\n",
            "================================================================================\n",
            "LIMPIEZA Y VALIDACIÓN DE DATOS\n",
            "================================================================================\n",
            "\n",
            "  [LIMPIEZA] AQEU...\n",
            "    Snapshots iniciales: 101,610\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:57,109 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n",
            "2025-12-09 14:23:57,111 - data_cleaner_module - INFO -       5308427: 101,559 snapshots ([VALID])\n",
            "2025-12-09 14:23:57,111 - data_cleaner_module - INFO -       5308428: 51 snapshots ([INVALID])\n",
            "2025-12-09 14:23:57,141 - data_cleaner_module - INFO -     [OK] Removed 51 non-trading snapshots (0.05%)\n",
            "2025-12-09 14:23:57,142 - data_cleaner_module - INFO -     [OK] Kept 101,559 continuous trading snapshots (99.95%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] Snapshots finales: 101,559 (99.95% retenido)\n",
            "\n",
            "  [LIMPIEZA] XMAD...\n",
            "    Snapshots iniciales: 364,903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:57,643 - data_cleaner_module - INFO -     Removed 1 magic numbers (0.00%)\n",
            "2025-12-09 14:23:57,646 - data_cleaner_module - INFO -     Códigos esperados para XMAD: [5832713, 5832756]\n",
            "2025-12-09 14:23:57,647 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(5832754), np.int64(5832755), np.int64(5832756), np.int64(5832757), np.int64(5832758), np.int64(5832762), np.int64(5832763)]\n",
            "2025-12-09 14:23:57,648 - data_cleaner_module - INFO -     Códigos que coinciden: [5832756]\n",
            "2025-12-09 14:23:57,808 - data_cleaner_module - INFO -     Snapshots con estado asignado: 364,901 (100.00%)\n",
            "2025-12-09 14:23:57,809 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 1 (0.00%)\n",
            "2025-12-09 14:23:58,613 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n",
            "2025-12-09 14:23:58,616 - data_cleaner_module - INFO -       5832756: 362,896 snapshots ([VALID])\n",
            "2025-12-09 14:23:58,617 - data_cleaner_module - INFO -       5832757: 1,393 snapshots ([INVALID])\n",
            "2025-12-09 14:23:58,618 - data_cleaner_module - INFO -       5832755: 609 snapshots ([INVALID])\n",
            "2025-12-09 14:23:58,619 - data_cleaner_module - INFO -       5832758: 2 snapshots ([INVALID])\n",
            "2025-12-09 14:23:58,620 - data_cleaner_module - INFO -       5832754: 1 snapshots ([INVALID])\n",
            "2025-12-09 14:23:58,800 - data_cleaner_module - INFO -     [OK] Removed 2,006 non-trading snapshots (0.55%)\n",
            "2025-12-09 14:23:58,802 - data_cleaner_module - INFO -     [OK] Kept 362,896 continuous trading snapshots (99.45%)\n",
            "2025-12-09 14:23:59,261 - data_cleaner_module - INFO -     Removed 2 invalid prices (0.00%)\n",
            "2025-12-09 14:23:59,855 - data_cleaner_module - INFO -     Códigos esperados para CEUX: [12255233]\n",
            "2025-12-09 14:23:59,856 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(12255233), np.int64(12255234), np.int64(12255235), np.int64(12255237), np.int64(12255244)]\n",
            "2025-12-09 14:23:59,857 - data_cleaner_module - INFO -     Códigos que coinciden: [12255233]\n",
            "2025-12-09 14:23:59,898 - data_cleaner_module - INFO -     Snapshots con estado asignado: 78,462 (100.00%)\n",
            "2025-12-09 14:23:59,898 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 0 (0.00%)\n",
            "2025-12-09 14:23:59,990 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] Snapshots finales: 362,894 (99.45% retenido)\n",
            "\n",
            "  [LIMPIEZA] CEUX...\n",
            "    Snapshots iniciales: 78,462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 14:23:59,991 - data_cleaner_module - INFO -       12255233: 78,375 snapshots ([VALID])\n",
            "2025-12-09 14:23:59,992 - data_cleaner_module - INFO -       12255237: 50 snapshots ([INVALID])\n",
            "2025-12-09 14:23:59,992 - data_cleaner_module - INFO -       12255244: 37 snapshots ([INVALID])\n",
            "2025-12-09 14:24:00,016 - data_cleaner_module - INFO -     [OK] Removed 87 non-trading snapshots (0.11%)\n",
            "2025-12-09 14:24:00,018 - data_cleaner_module - INFO -     [OK] Kept 78,375 continuous trading snapshots (99.89%)\n",
            "2025-12-09 14:24:00,161 - data_cleaner_module - INFO -     Códigos esperados para TQEX: [7608181]\n",
            "2025-12-09 14:24:00,162 - data_cleaner_module - INFO -     Códigos encontrados en STS: [np.int64(7608181), np.int64(7608182), np.int64(7608183)]\n",
            "2025-12-09 14:24:00,162 - data_cleaner_module - INFO -     Códigos que coinciden: [7608181]\n",
            "2025-12-09 14:24:00,179 - data_cleaner_module - INFO -     Snapshots con estado asignado: 43,316 (100.00%)\n",
            "2025-12-09 14:24:00,180 - data_cleaner_module - INFO -     Snapshots sin estado asignado: 0 (0.00%)\n",
            "2025-12-09 14:24:00,216 - data_cleaner_module - INFO -     Distribución de estados encontrados:\n",
            "2025-12-09 14:24:00,217 - data_cleaner_module - INFO -       7608181: 43,316 snapshots ([VALID])\n",
            "2025-12-09 14:24:00,304 - data_cleaner_module - INFO - Quality metrics: 588,291 → 586,144 (99.64% retained)\n",
            "2025-12-09 14:24:00,353 - consolidator_module - INFO - ConsolidatedTape initialized: time_bin=100ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [OK] Snapshots finales: 78,375 (99.89% retenido)\n",
            "\n",
            "  [LIMPIEZA] TQEX...\n",
            "    Snapshots iniciales: 43,316\n",
            "    [OK] Snapshots finales: 43,316 (100.00% retenido)\n",
            "\n",
            "  [MÉTRICAS DE CALIDAD AGREGADAS]\n",
            "    Filas originales: 588,291\n",
            "    Eliminadas por magic numbers: 1 (0.00%)\n",
            "    Eliminadas por status inválido: 2,144 (0.36%)\n",
            "    Eliminadas por validaciones: 2 (0.00%)\n",
            "    Filas finales: 586,144 (99.64% retenido)\n",
            "\n",
            "[EXITO] Limpieza completada para 4 venues\n",
            "\n",
            "================================================================================\n",
            "CREANDO CONSOLIDATED TAPE\n",
            "================================================================================\n",
            "  Venues a consolidar: ['AQEU', 'XMAD', 'CEUX', 'TQEX']\n",
            "   AQEU: 306 snapshots preparados\n",
            "   XMAD: 307 snapshots preparados\n",
            "   CEUX: 306 snapshots preparados\n",
            "   TQEX: 307 snapshots preparados\n",
            "\n",
            "  Usando XMAD como base (307 rows)\n",
            "    Merging con TQEX... OK (307 rows)\n",
            "    Merging con AQEU... OK (307 rows)\n",
            "    Merging con CEUX... OK (307 rows)\n",
            "\n",
            "  [OK] Tape consolidado creado: (307, 17)\n",
            "    - Timestamps únicos: 307\n",
            "    - Columnas totales: 17\n",
            "\n",
            "  Aplicando forward fill...\n",
            "    NaNs antes: 0\n",
            "    NaNs después: 0\n",
            "\n",
            "  Tape final: (307, 17)\n",
            "\n",
            "  [ESTADISTICAS DE COBERTURA]\n",
            "    AQEU: 307 rows válidas (100.0% cobertura)\n",
            "    XMAD: 307 rows válidas (100.0% cobertura)\n",
            "    CEUX: 307 rows válidas (100.0% cobertura)\n",
            "    TQEX: 307 rows válidas (100.0% cobertura)\n",
            "  No arbitrage opportunities found for ES0113900J37\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ANÁLISIS COMPLETO DE TODOS LOS ISINs\n",
        "# ============================================================================\n",
        "# Igual que el otro código: procesar todos los ISINs y generar Money Table\n",
        "\n",
        "from collections import defaultdict\n",
        "from latency_simulator_module import simulate_latency_with_losses\n",
        "\n",
        "# Configuración (igual que el otro código)\n",
        "DATE = \"2025-11-07\"  # Ajustar según tus datos\n",
        "LATENCY_LEVELS = config.LATENCY_BUCKETS  # [0, 100, 500, 1000, ...]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ANÁLISIS COMPLETO DE TODOS LOS ISINs\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Descubrir todos los ISINs\n",
        "print(\"\\n[INFO] Descubriendo ISINs...\")\n",
        "all_isins = loader.discover_isins()\n",
        "print(f\"Found {len(all_isins)} unique ISINs\\n\")\n",
        "\n",
        "# Storage for results (igual que el otro código)\n",
        "money_table_data = []\n",
        "\n",
        "# Process each ISIN (igual que el otro código)\n",
        "for isin_idx, isin in enumerate(all_isins, 1):\n",
        "    print(f\"[{isin_idx}/{len(all_isins)}] Processing ISIN: {isin}\")\n",
        "    \n",
        "    # Load data\n",
        "    data_dict_raw = loader.load_isin_data(isin)\n",
        "    \n",
        "    if not data_dict_raw:\n",
        "        print(f\"  No data found for {isin}\")\n",
        "        continue\n",
        "    \n",
        "    # Convertir formato: nuestro código usa {'mic': {'qte': df, 'sts': df}}\n",
        "    # El otro código usa {'mic': (qte_df, sts_df)}\n",
        "    data_dict = {}\n",
        "    for mic, venue_data in data_dict_raw.items():\n",
        "        qte_df = venue_data.get('qte', pd.DataFrame())\n",
        "        sts_df = venue_data.get('sts', pd.DataFrame())\n",
        "        if not qte_df.empty:\n",
        "            data_dict[mic] = (qte_df, sts_df)\n",
        "    \n",
        "    if not data_dict:\n",
        "        print(f\"  No valid data found for {isin}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"  Found data from {len(data_dict)} exchange(s): {list(data_dict.keys())}\")\n",
        "    \n",
        "    # Clean data (aplicar limpieza)\n",
        "    cleaner = DataCleaner()\n",
        "    clean_data = cleaner.clean_all_venues(data_dict_raw)\n",
        "    \n",
        "    if not clean_data:\n",
        "        print(f\"  No data after cleaning for {isin}\")\n",
        "        continue\n",
        "    \n",
        "    # Create consolidated tape\n",
        "    tape_builder = ConsolidatedTape(time_bin_ms=100)\n",
        "    consolidated = tape_builder.create_tape(clean_data)\n",
        "    \n",
        "    if consolidated is None or consolidated.empty:\n",
        "        print(f\"  No consolidated tape created for {isin}\")\n",
        "        continue\n",
        "    \n",
        "    # Detect arbitrage opportunities (igual que el otro código)\n",
        "    signal_gen = SignalGenerator()\n",
        "    opportunities = signal_gen.detect_arbitrage_opportunities(consolidated)\n",
        "    \n",
        "    if opportunities.empty:\n",
        "        print(f\"  No arbitrage opportunities found for {isin}\")\n",
        "        continue\n",
        "    \n",
        "    # Apply rising edge detection (igual que el otro código)\n",
        "    opportunities = signal_gen.apply_rising_edge(opportunities)\n",
        "    \n",
        "    # Filtrar solo las que pasaron el rising edge\n",
        "    opportunities = opportunities[opportunities.get('is_rising_edge', False)].copy()\n",
        "    \n",
        "    if opportunities.empty:\n",
        "        print(f\"  No opportunities after rising edge for {isin}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"  Found {len(opportunities)} arbitrage opportunities (after rising edge)\")\n",
        "    \n",
        "    # Simulate latency for each level (igual que el otro código)\n",
        "    profits_by_latency = {}\n",
        "    for latency_us in LATENCY_LEVELS:\n",
        "        profit = simulate_latency_with_losses(opportunities, consolidated, latency_us)\n",
        "        profits_by_latency[latency_us] = profit\n",
        "        \n",
        "        money_table_data.append({\n",
        "            'ISIN': isin,\n",
        "            'Latency_us': latency_us,\n",
        "            'Profit_EUR': profit\n",
        "        })\n",
        "    \n",
        "    # Print profits for all latencies for this ISIN (igual que el otro código)\n",
        "    print(f\"\\n  Profits by Latency for {isin}:\")\n",
        "    print(f\"  {'Latency (µs)':<15} {'Latency (ms)':<15} {'Profit/Loss (€)':<20} {'% of 0 latency':<15}\")\n",
        "    print(f\"  {'-'*65}\")\n",
        "    \n",
        "    zero_latency_profit = profits_by_latency[0]\n",
        "    for latency_us in LATENCY_LEVELS:\n",
        "        profit = profits_by_latency[latency_us]\n",
        "        latency_ms = latency_us / 1000\n",
        "        if zero_latency_profit != 0:\n",
        "            pct = (profit / zero_latency_profit * 100) if zero_latency_profit != 0 else 0\n",
        "            pct_str = f\"{pct:.1f}%\"\n",
        "        else:\n",
        "            pct_str = \"N/A\"\n",
        "        \n",
        "        # Color coding: negative = loss, positive = profit\n",
        "        profit_str = f\"€{profit:,.2f}\"\n",
        "        if profit < 0:\n",
        "            profit_str = f\"€{profit:,.2f} (LOSS)\"\n",
        "        \n",
        "        print(f\"  {latency_us:<15} {latency_ms:<15.3f} {profit_str:<20} {pct_str:<15}\")\n",
        "    \n",
        "    print(f\"  {'-'*65}\")\n",
        "    print(f\"  Total profit at 0 latency: €{zero_latency_profit:,.2f}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverable 1: The \"Money Table\"\n",
        "\n",
        "Tabla pivot con profit por ISIN y latencia (igual que el otro código)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data available for Money Table.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DELIVERABLE 1: THE \"MONEY TABLE\"\n",
        "# ============================================================================\n",
        "# Igual que el otro código\n",
        "\n",
        "if money_table_data:\n",
        "    money_df = pd.DataFrame(money_table_data)\n",
        "    \n",
        "    # Create pivot table\n",
        "    pivot = money_df.pivot_table(\n",
        "        index='ISIN',\n",
        "        columns='Latency_us',\n",
        "        values='Profit_EUR',\n",
        "        aggfunc='sum'\n",
        "    )\n",
        "    \n",
        "    # Add TOTAL row\n",
        "    pivot.loc['TOTAL'] = pivot.sum()\n",
        "    \n",
        "    print(\"MONEY TABLE: Total Realized Profit/Loss by ISIN and Latency\")\n",
        "    print(\"=\"*120)\n",
        "    print(pivot.to_string())\n",
        "    \n",
        "    # Summary by latency\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY BY LATENCY (All ISINs Combined)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Calculate profit at 0 latency for percentage calculation\n",
        "    profit_at_zero = pivot.loc['TOTAL', 0] if 0 in pivot.columns else 0\n",
        "    \n",
        "    summary_df = pd.DataFrame({\n",
        "        'Latency (µs)': LATENCY_LEVELS,\n",
        "        'Total Profit/Loss (€)': [pivot.loc['TOTAL', lat] for lat in LATENCY_LEVELS],\n",
        "        'Latency (ms)': [lat / 1000 for lat in LATENCY_LEVELS],\n",
        "        '% of 0 latency': [\n",
        "            (pivot.loc['TOTAL', lat] / profit_at_zero * 100) if profit_at_zero != 0 else 0 \n",
        "            for lat in LATENCY_LEVELS\n",
        "        ]\n",
        "    })\n",
        "    \n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "    # Count arbitrage opportunities by exchange direction (Buy -> Sell)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ARBITRAGE OPPORTUNITIES BY EXCHANGE DIRECTION\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Get ISINs with opportunities at 0 latency\n",
        "    isins_with_opps = money_df[(money_df['Latency_us'] == 0) & (money_df['Profit_EUR'] > 0)]['ISIN'].unique()\n",
        "    \n",
        "    # Count opportunities by exchange direction (Buy Exchange -> Sell Exchange)\n",
        "    exchange_direction_counts = defaultdict(int)\n",
        "    \n",
        "    for isin in isins_with_opps:\n",
        "        # Load data for this ISIN\n",
        "        data_dict_raw = loader.load_isin_data(isin)\n",
        "        \n",
        "        if not data_dict_raw:\n",
        "            continue\n",
        "        \n",
        "        # Clean and consolidate\n",
        "        cleaner = DataCleaner()\n",
        "        clean_data = cleaner.clean_all_venues(data_dict_raw)\n",
        "        \n",
        "        if not clean_data:\n",
        "            continue\n",
        "        \n",
        "        tape_builder = ConsolidatedTape(time_bin_ms=100)\n",
        "        consolidated = tape_builder.create_tape(clean_data)\n",
        "        \n",
        "        if consolidated.empty:\n",
        "            continue\n",
        "        \n",
        "        # Detect arbitrage opportunities\n",
        "        signal_gen = SignalGenerator()\n",
        "        opportunities = signal_gen.detect_arbitrage_opportunities(consolidated)\n",
        "        \n",
        "        if opportunities.empty:\n",
        "            continue\n",
        "        \n",
        "        # Apply rising edge detection\n",
        "        opportunities = signal_gen.apply_rising_edge(opportunities)\n",
        "        opportunities = opportunities[opportunities.get('is_rising_edge', False)].copy()\n",
        "        \n",
        "        if opportunities.empty:\n",
        "            continue\n",
        "        \n",
        "        # Count by exchange direction (Buy -> Sell)\n",
        "        for _, opp in opportunities.iterrows():\n",
        "            buy_ex = opp.get('buy_exchange', '')\n",
        "            sell_ex = opp.get('sell_exchange', '')\n",
        "            \n",
        "            if buy_ex and sell_ex:\n",
        "                # Count direction separately (Buy -> Sell)\n",
        "                direction = f\"{buy_ex} → {sell_ex}\"\n",
        "                exchange_direction_counts[direction] += 1\n",
        "    \n",
        "    # Sort by count (descending)\n",
        "    sorted_directions = sorted(exchange_direction_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    if sorted_directions:\n",
        "        print(f\"\\n{'Rank':<8} {'Exchange Direction (Buy → Sell)':<40} {'Opportunities':<15}\")\n",
        "        print(f\"{'-'*8} {'-'*40} {'-'*15}\")\n",
        "        \n",
        "        for rank, (direction, count) in enumerate(sorted_directions, 1):\n",
        "            print(f\"{rank:<8} {direction:<40} {count:<15}\")\n",
        "        \n",
        "        print(f\"\\nTotal unique exchange directions: {len(sorted_directions)}\")\n",
        "        print(f\"Total opportunities: {sum(exchange_direction_counts.values())}\")\n",
        "    else:\n",
        "        print(\"No exchange direction data available.\")\n",
        "    \n",
        "else:\n",
        "    print(\"No data available for Money Table.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverable 2: The Decay Chart\n",
        "\n",
        "Gráfico de decay de profit con latencia (igual que el otro código)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data available for Decay Chart.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DELIVERABLE 2: THE DECAY CHART\n",
        "# ============================================================================\n",
        "# Igual que el otro código\n",
        "\n",
        "if money_table_data:\n",
        "    money_df = pd.DataFrame(money_table_data)\n",
        "    \n",
        "    # Calculate totals by latency\n",
        "    totals_by_latency = money_df.groupby('Latency_us')['Profit_EUR'].sum()\n",
        "    \n",
        "    latencies_ms = [lat / 1000 for lat in LATENCY_LEVELS]\n",
        "    profits = [totals_by_latency.get(lat, 0) for lat in LATENCY_LEVELS]\n",
        "    \n",
        "    max_profit = profits[0]\n",
        "    \n",
        "    # Check if there are any losses\n",
        "    has_losses = any(p < 0 for p in profits)\n",
        "    \n",
        "    # Calculate percentages\n",
        "    percentages = [(p / max_profit * 100) if max_profit != 0 else 0 for p in profits]\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Plot 1: Log scale (if all profits are positive)\n",
        "    if not has_losses and all(p > 0 for p in profits):\n",
        "        ax1.semilogy(latencies_ms, profits, 'b-o', linewidth=2, markersize=8)\n",
        "        ax1.set_ylabel('Profit (€, log scale)', fontsize=12)\n",
        "        ax1.set_title('Profit Decay with Latency (Log Scale)', fontsize=14, fontweight='bold')\n",
        "    else:\n",
        "        ax1.plot(latencies_ms, profits, 'b-o', linewidth=2, markersize=8)\n",
        "        ax1.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Break-even')\n",
        "        ax1.set_ylabel('Profit/Loss (€)', fontsize=12)\n",
        "        ax1.set_title('Profit Decay with Latency (Linear Scale)', fontsize=14, fontweight='bold')\n",
        "        ax1.legend()\n",
        "    \n",
        "    ax1.set_xlabel('Latency (ms)', fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Percentage of 0 latency profit\n",
        "    ax2.plot(latencies_ms, percentages, 'g-o', linewidth=2, markersize=8)\n",
        "    ax2.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Break-even')\n",
        "    ax2.axhline(y=100, color='b', linestyle=':', linewidth=1, alpha=0.5, label='100% (0 latency)')\n",
        "    ax2.set_xlabel('Latency (ms)', fontsize=12)\n",
        "    ax2.set_ylabel('% of Profit at 0 Latency', fontsize=12)\n",
        "    ax2.set_title('Profit Decay as % of 0 Latency', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(config.FIGURES_DIR / 'decay_chart.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print decay analysis\n",
        "    print(\"\\nDecay Analysis:\")\n",
        "    print(f\"  Maximum profit (0 latency): €{max_profit:,.2f}\")\n",
        "    \n",
        "    # Find profits at key latencies\n",
        "    key_latencies = [1000, 10000, 100000]  # 1ms, 10ms, 100ms\n",
        "    for lat_us in key_latencies:\n",
        "        if lat_us in totals_by_latency.index:\n",
        "            profit_at_latency = totals_by_latency[lat_us]\n",
        "            print(f\"  Profit/Loss at {lat_us/1000}ms: €{profit_at_latency:,.2f}\")\n",
        "else:\n",
        "    print(\"No data available for Decay Chart.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverable 3: Top Opportunities\n",
        "\n",
        "Top 5 ISINs más rentables con detalles (igual que el otro código)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data available for Top Opportunities.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DELIVERABLE 3: TOP OPPORTUNITIES\n",
        "# ============================================================================\n",
        "# Igual que el otro código\n",
        "\n",
        "if money_table_data:\n",
        "    money_df = pd.DataFrame(money_table_data)\n",
        "    \n",
        "    # Get top 5 ISINs by profit at 0 latency\n",
        "    zero_latency = money_df[money_df['Latency_us'] == 0]\n",
        "    top_isins = zero_latency.nlargest(5, 'Profit_EUR')\n",
        "    \n",
        "    print(\"TOP 5 MOST PROFITABLE ISINs (at 0 latency)\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "    \n",
        "    for rank, (_, row) in enumerate(top_isins.iterrows(), 1):\n",
        "        isin = row['ISIN']\n",
        "        total_profit = row['Profit_EUR']\n",
        "        \n",
        "        # Reload data to get detailed info\n",
        "        data_dict_raw = loader.load_isin_data(isin)\n",
        "        cleaner = DataCleaner()\n",
        "        clean_data = cleaner.clean_all_venues(data_dict_raw)\n",
        "        tape_builder = ConsolidatedTape(time_bin_ms=100)\n",
        "        consolidated = tape_builder.create_tape(clean_data)\n",
        "        signal_gen = SignalGenerator()\n",
        "        opportunities = signal_gen.detect_arbitrage_opportunities(consolidated)\n",
        "        opportunities = signal_gen.apply_rising_edge(opportunities)\n",
        "        opportunities = opportunities[opportunities.get('is_rising_edge', False)].copy()\n",
        "        \n",
        "        if opportunities.empty:\n",
        "            continue\n",
        "        \n",
        "        num_opps = len(opportunities)\n",
        "        avg_profit = opportunities['total_profit'].mean()\n",
        "        max_profit = opportunities['total_profit'].max()\n",
        "        total_qty = opportunities['tradeable_qty'].sum()\n",
        "        \n",
        "        # Find best opportunity\n",
        "        best_opp = opportunities.loc[opportunities['total_profit'].idxmax()]\n",
        "        \n",
        "        print(f\"{rank}. ISIN: {isin}\")\n",
        "        print(f\"   Total Theoretical Profit: €{total_profit:,.2f}\")\n",
        "        print(f\"   Number of opportunities: {num_opps}\")\n",
        "        print(f\"   Average profit per opportunity: €{avg_profit:,.2f}\")\n",
        "        print(f\"   Max profit per opportunity: €{max_profit:,.2f}\")\n",
        "        print(f\"   Total tradeable quantity: {total_qty:,.0f} shares\")\n",
        "        print()\n",
        "        print(f\"   Best Opportunity:\")\n",
        "        print(f\"     Buy at: {best_opp['buy_exchange']} @ €{best_opp['buy_price']:.4f}\")\n",
        "        print(f\"     Sell at: {best_opp['sell_exchange']} @ €{best_opp['sell_price']:.4f}\")\n",
        "        print(f\"     Profit per share: €{best_opp['profit_per_share']:.4f}\")\n",
        "        print(f\"     Quantity: {best_opp['tradeable_qty']:.0f} shares\")\n",
        "        print(f\"     Total profit: €{best_opp['total_profit']:.2f}\")\n",
        "        print()\n",
        "    \n",
        "    # Summary table\n",
        "    print(\"=\"*80)\n",
        "    print(\"TOP 5 SUMMARY TABLE\")\n",
        "    print(\"=\"*80)\n",
        "    summary_table = pd.DataFrame({\n",
        "        'ISIN': top_isins['ISIN'].values,\n",
        "        'Total Profit at 0 Latency (€)': [f\"€{p:,.2f}\" for p in top_isins['Profit_EUR'].values]\n",
        "    })\n",
        "    print(summary_table.to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Sanity checks\n",
        "    print(\"=\"*80)\n",
        "    print(\"SANITY CHECKS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"✓ Checking if profits are reasonable...\")\n",
        "    print()\n",
        "    \n",
        "    for _, row in top_isins.iterrows():\n",
        "        isin = row['ISIN']\n",
        "        \n",
        "        # Reload to get average price\n",
        "        data_dict_raw = loader.load_isin_data(isin)\n",
        "        cleaner = DataCleaner()\n",
        "        clean_data = cleaner.clean_all_venues(data_dict_raw)\n",
        "        tape_builder = ConsolidatedTape(time_bin_ms=100)\n",
        "        consolidated = tape_builder.create_tape(clean_data)\n",
        "        signal_gen = SignalGenerator()\n",
        "        opportunities = signal_gen.detect_arbitrage_opportunities(consolidated)\n",
        "        opportunities = signal_gen.apply_rising_edge(opportunities)\n",
        "        opportunities = opportunities[opportunities.get('is_rising_edge', False)].copy()\n",
        "        \n",
        "        if opportunities.empty:\n",
        "            continue\n",
        "        \n",
        "        avg_price = (opportunities['buy_price'].mean() + opportunities['sell_price'].mean()) / 2\n",
        "        avg_profit_per_share = opportunities['profit_per_share'].mean()\n",
        "        avg_profit_pct = (avg_profit_per_share / avg_price) * 100\n",
        "        \n",
        "        print(f\"{isin}:\")\n",
        "        print(f\"  Average price: €{avg_price:.4f}\")\n",
        "        print(f\"  Average profit per share: €{avg_profit_per_share:.4f}\")\n",
        "        print(f\"  Average profit %: {avg_profit_pct:.4f}%\")\n",
        "        \n",
        "        if avg_profit_pct < 1.0:\n",
        "            print(f\"  ✓ Profit percentage looks reasonable (<1%)\")\n",
        "        else:\n",
        "            print(f\"  ⚠ Profit percentage seems high (>1%)\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No data available for Top Opportunities.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Answers to Key Questions\n",
        "\n",
        "Respuestas a las 3 preguntas clave (igual que el otro código)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data available for summary.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SUMMARY & ANSWERS TO KEY QUESTIONS\n",
        "# ============================================================================\n",
        "# Igual que el otro código\n",
        "\n",
        "if money_table_data:\n",
        "    money_df = pd.DataFrame(money_table_data)\n",
        "    \n",
        "    # Calculate totals\n",
        "    totals_by_latency = money_df.groupby('Latency_us')['Profit_EUR'].sum()\n",
        "    \n",
        "    max_profit = totals_by_latency.get(0, 0)\n",
        "    profit_1ms = totals_by_latency.get(1000, 0)\n",
        "    profit_10ms = totals_by_latency.get(10000, 0)\n",
        "    profit_100ms = totals_by_latency.get(100000, 0)\n",
        "    \n",
        "    # Count ISINs with opportunities\n",
        "    isins_with_opps = money_df[money_df['Latency_us'] == 0]\n",
        "    isins_with_opps = isins_with_opps[isins_with_opps['Profit_EUR'] > 0]\n",
        "    num_isins = len(isins_with_opps)\n",
        "    \n",
        "    # Calculate half-life (50% profit remaining)\n",
        "    half_profit = max_profit / 2\n",
        "    half_life_ms = None\n",
        "    \n",
        "    for lat_us in LATENCY_LEVELS:\n",
        "        profit = totals_by_latency.get(lat_us, 0)\n",
        "        if profit <= half_profit:\n",
        "            half_life_ms = lat_us / 1000\n",
        "            break\n",
        "    \n",
        "    if half_life_ms is None:\n",
        "        half_life_ms = 100.0  # Default if not reached\n",
        "    \n",
        "    # Check for losses\n",
        "    has_losses = any(p < 0 for p in totals_by_latency.values)\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"ANSWERS TO KEY QUESTIONS\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "    \n",
        "    print(\"1. Do arbitrage opportunities still exist in Spanish equities?\")\n",
        "    if max_profit > 0:\n",
        "        print(f\"   ✓ YES! Found arbitrage opportunities with total theoretical profit of €{max_profit:,.2f}\")\n",
        "        print(f\"   ✓ Number of ISINs with opportunities: {num_isins}\")\n",
        "    else:\n",
        "        print(\"   ✗ NO arbitrage opportunities found.\")\n",
        "    print()\n",
        "    \n",
        "    print(\"2. What is the maximum theoretical profit (assuming 0 latency)?\")\n",
        "    print(f\"   Maximum theoretical profit: €{max_profit:,.2f}\")\n",
        "    if num_isins > 0:\n",
        "        top_isin = isins_with_opps.nlargest(1, 'Profit_EUR').iloc[0]\n",
        "        print(f\"   Top ISIN: {top_isin['ISIN']} with €{top_isin['Profit_EUR']:,.2f}\")\n",
        "    print()\n",
        "    \n",
        "    print(\"3. The 'Latency Decay' Curve: How quickly does profit vanish?\")\n",
        "    print(f\"   At 0µs (0ms):     €{max_profit:,.2f} (100.0%)\")\n",
        "    if profit_1ms is not None:\n",
        "        pct_1ms = (profit_1ms / max_profit * 100) if max_profit != 0 else 0\n",
        "        print(f\"   At 1,000µs (1ms):  €{profit_1ms:,.2f} ({pct_1ms:.1f}%)\")\n",
        "    if profit_10ms is not None:\n",
        "        pct_10ms = (profit_10ms / max_profit * 100) if max_profit != 0 else 0\n",
        "        print(f\"   At 10,000µs (10ms): €{profit_10ms:,.2f} ({pct_10ms:.1f}%)\")\n",
        "    if profit_100ms is not None:\n",
        "        pct_100ms = (profit_100ms / max_profit * 100) if max_profit != 0 else 0\n",
        "        print(f\"   At 100,000µs (100ms): €{profit_100ms:,.2f} ({pct_100ms:.1f}%)\")\n",
        "    print()\n",
        "    print(f\"   Half-life (50% profit remaining): ~{half_life_ms:.1f}ms\")\n",
        "    print()\n",
        "    \n",
        "    if has_losses:\n",
        "        print(\"   ⚠ WARNING: Some latencies resulted in losses (negative profits).\")\n",
        "        print(\"     This indicates that arbitrage opportunities can turn into losses with latency.\")\n",
        "        print()\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"ANALYSIS COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"No data available for summary.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
